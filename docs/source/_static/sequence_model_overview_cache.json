{"yoso": "## Overview\n\nThe YOSO model was proposed in [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714)  \nby Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh. YOSO approximates standard softmax self-attention\nvia a Bernoulli sampling scheme based on Locality Sensitive Hashing (LSH). In principle, all the Bernoulli random variables can be sampled with\na single hash. \n\nThe abstract from the paper is the following:\n\n*Transformer-based models are widely used in natural language processing (NLP). Central to the transformer model is \nthe self-attention mechanism, which captures the interactions of token pairs in the input sequences and depends quadratically \non the sequence length. Training such models on longer sequences is expensive. In this paper, we show that a Bernoulli sampling \nattention mechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic complexity of such models to linear. \nWe bypass the quadratic cost by considering self-attention as a sum of individual tokens associated with Bernoulli random \nvariables that can, in principle, be sampled at once by a single hash (although in practice, this number may be a small constant). \nThis leads to an efficient sampling scheme to estimate self-attention which relies on specific modifications of \nLSH (to enable deployment on GPU architectures). We evaluate our algorithm on the GLUE benchmark with standard 512 sequence \nlength where we see favorable performance relative to a standard pretrained Transformer. On the Long Range Arena (LRA) benchmark, \nfor evaluating performance on long sequences, our method achieves results consistent with softmax self-attention but with sizable \nspeed-ups and memory savings and often outperforms other efficient self-attention methods. Our code is available at this https URL*\n\nTips:\n\n- The YOSO attention algorithm is implemented through custom CUDA kernels, functions written in CUDA C++ that can be executed multiple times\nin parallel on a GPU.\n- The kernels provide a `fast_hash` function, which approximates the random projections of the queries and keys using the Fast Hadamard Transform. Using these\nhash codes, the `lsh_cumulation` function approximates self-attention via LSH-based Bernoulli sampling.\n- To use the custom kernels, the user should set `config.use_expectation = False`. To ensure that the kernels are compiled successfully, \nthe user must install the correct version of PyTorch and cudatoolkit. By default, `config.use_expectation = True`, which uses YOSO-E and \ndoes not require compiling CUDA kernels.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/yoso_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> YOSO Attention Algorithm. Taken from the <a href=\"https://arxiv.org/abs/2111.09714\">original paper</a>.</small>\n\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/YOSO).\n\n", "albert": "## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n\n- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:\n\n*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.*\n\nTips:\n\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n  than the left.\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n  number of (repeating) layers.\n- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.\n- Layers are split in groups that share parameters (to save memory).\nNext sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n", "bart": "## Overview\n\nThe Bart model was proposed in [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer on 29 Oct, 2019.\n\nAccording to the abstract,\n\n- Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a\n  left-to-right decoder (like GPT).\n- The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme,\n  where spans of text are replaced with a single mask token.\n- BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It\n  matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new\n  state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains\n  of up to 6 ROUGE.\n\nTips:\n\n- BART is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n- Sequence-to-sequence model with an encoder and a decoder. Encoder is fed a corrupted version of the tokens, decoder is fed the original tokens (but has a mask to hide the future words like a regular transformers decoder). A composition of the following transformations are applied on the pretraining tasks for the encoder:\n\n  * mask random tokens (like in BERT)\n  * delete random tokens\n  * mask a span of k tokens with a single mask token (a span of 0 tokens is an insertion of a mask token)\n  * permute sentences\n  * rotate the document to make it start at a specific token\n\nThis model was contributed by [sshleifer](https://huggingface.co/sshleifer). The Authors' code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/bart).\n\n\n#", "bert": "## Overview\n\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\nThe abstract from the paper is the following:\n\n*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.*\n\n*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).*\n\nTips:\n\n- BERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n- BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation.\n- Corrupts the inputs by using random masking, more precisely, during pretraining, a given percentage of tokens (usually 15%) is masked by:\n\n    * a special mask token with probability 0.8\n    * a random token different from the one masked with probability 0.1\n    * the same token with probability 0.1\n    \n- The model must predict the original sentence, but has a second objective: inputs are two sentences A and B (with a separation token in between). With probability 50%, the sentences are consecutive in the corpus, in the remaining 50% they are not related. The model has to predict if the sentences are consecutive or not.\n\n\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/google-research/bert).\n\n", "bert-generation": "## Overview\n\nThe BertGeneration model is a BERT model that can be leveraged for sequence-to-sequence tasks using\n[`EncoderDecoderModel`] as proposed in [Leveraging Pre-trained Checkpoints for Sequence Generation\nTasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n\nThe abstract from the paper is the following:\n\n*Unsupervised pretraining of large neural models has recently revolutionized Natural Language Processing. By\nwarm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple\nbenchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language\nUnderstanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We\ndeveloped a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT,\nGPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both\nencoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation,\nText Summarization, Sentence Splitting, and Sentence Fusion.*\n\nUsage:\n\n- The model can be used in combination with the [`EncoderDecoderModel`] to leverage two pretrained\n  BERT checkpoints for subsequent fine-tuning.\n\n```python\n>>> # leverage checkpoints for Bert2Bert model...\n>>> # use BERT's cls token as BOS token and sep token as EOS token\n>>> encoder = BertGenerationEncoder.from_pretrained(\"bert-large-uncased\", bos_token_id=101, eos_token_id=102)\n>>> # add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n>>> decoder = BertGenerationDecoder.from_pretrained(\n...     \"bert-large-uncased\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n... )\n>>> bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n\n>>> # create tokenizer...\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n\n>>> input_ids = tokenizer(\n...     \"This is a long article to summarize\", add_special_tokens=False, return_tensors=\"pt\"\n... ).input_ids\n>>> labels = tokenizer(\"This is a short summary\", return_tensors=\"pt\").input_ids\n\n>>> # train...\n>>> loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss\n>>> loss.backward()\n```\n\n- Pretrained [`EncoderDecoderModel`] are also directly available in the model hub, e.g.,\n\n\n```python\n>>> # instantiate sentence fusion model\n>>> sentence_fuser = EncoderDecoderModel.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n\n>>> input_ids = tokenizer(\n...     \"This is the first sentence. This is the second sentence.\", add_special_tokens=False, return_tensors=\"pt\"\n... ).input_ids\n\n>>> outputs = sentence_fuser.generate(input_ids)\n\n>>> print(tokenizer.decode(outputs[0]))\n```\n\nTips:\n\n- [`BertGenerationEncoder`] and [`BertGenerationDecoder`] should be used in\n  combination with [`EncoderDecoder`].\n- For summarization, sentence splitting, sentence fusion and translation, no special tokens are required for the input.\n  Therefore, no EOS token should be added to the end of the input.\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://tfhub.dev/s?module-type=text-generation&subtype=module,placeholder).\n\n", "big_bird": "## Overview\n\nThe BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by\nZaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon,\nSantiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others. BigBird, is a sparse-attention\nbased transformer which extends Transformer based models, such as BERT to much longer sequences. In addition to sparse\nattention, BigBird also applies global attention as well as random attention to the input sequence. Theoretically, it\nhas been shown that applying sparse, global, and random attention approximates full attention, while being\ncomputationally much more efficient for longer sequences. As a consequence of the capability to handle longer context,\nBigBird has shown improved performance on various long document NLP tasks, such as question answering and\nsummarization, compared to BERT or RoBERTa.\n\nThe abstract from the paper is the following:\n\n*Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP.\nUnfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence\nlength due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that\nreduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and\nis Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our\ntheoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire\nsequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to\n8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context,\nBigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also\npropose novel applications to genomics data.*\n\nTips:\n\n- For an in-detail explanation on how BigBird's attention works, see [this blog post](https://huggingface.co/blog/big-bird).\n- BigBird comes with 2 implementations: **original_full** & **block_sparse**. For the sequence length < 1024, using\n  **original_full** is advised as there is no benefit in using **block_sparse** attention.\n- The code currently uses window size of 3 blocks and 2 global blocks.\n- Sequence length must be divisible by block size.\n- Current implementation supports only **ITC**.\n- Current implementation doesn't support **num_random_blocks = 0**\n- BigBird is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n\nThis model was contributed by [vasudevgupta](https://huggingface.co/vasudevgupta). The original code can be found\n[here](https://github.com/google-research/bigbird).\n\n", "bigbird_pegasus": "## Overview\n\nThe BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by\nZaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon,\nSantiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others. BigBird, is a sparse-attention\nbased transformer which extends Transformer based models, such as BERT to much longer sequences. In addition to sparse\nattention, BigBird also applies global attention as well as random attention to the input sequence. Theoretically, it\nhas been shown that applying sparse, global, and random attention approximates full attention, while being\ncomputationally much more efficient for longer sequences. As a consequence of the capability to handle longer context,\nBigBird has shown improved performance on various long document NLP tasks, such as question answering and\nsummarization, compared to BERT or RoBERTa.\n\nThe abstract from the paper is the following:\n\n*Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP.\nUnfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence\nlength due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that\nreduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and\nis Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our\ntheoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire\nsequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to\n8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context,\nBigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also\npropose novel applications to genomics data.*\n\nTips:\n\n- For an in-detail explanation on how BigBird's attention works, see [this blog post](https://huggingface.co/blog/big-bird).\n- BigBird comes with 2 implementations: **original_full** & **block_sparse**. For the sequence length < 1024, using\n  **original_full** is advised as there is no benefit in using **block_sparse** attention.\n- The code currently uses window size of 3 blocks and 2 global blocks.\n- Sequence length must be divisible by block size.\n- Current implementation supports only **ITC**.\n- Current implementation doesn't support **num_random_blocks = 0**.\n- BigBirdPegasus uses the [PegasusTokenizer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/pegasus/tokenization_pegasus.py).\n- BigBird is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n\nThe original code can be found [here](https://github.com/google-research/bigbird).\n\n", "biogpt": "## Overview\n\nThe BioGPT model was proposed in [BioGPT: generative pre-trained transformer for biomedical text generation and mining\n](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu. BioGPT is a domain-specific generative pre-trained Transformer language model for biomedical text generation and mining. BioGPT follows the Transformer language model backbone, and is pre-trained on 15M PubMed abstracts from scratch.\n\nThe abstract from the paper is the following:\n\n*Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.*\n\nTips:\n\n- BioGPT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n- BioGPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows BioGPT to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n- The model can take the `past_key_values` (for PyTorch) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the BioGptForCausalLM.forward() method for more information on its usage.\n\nThis model was contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/microsoft/BioGPT).\n\n", "blenderbot": "## Overview\n\nThe Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\nJing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston on 30 Apr 2020.\n\nThe abstract of the paper is the following:\n\n*Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that\nscaling neural models in the number of parameters and the size of the data they are trained on gives improved results,\nwe show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of\nskills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to\ntheir partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent\npersona. We show that large scale models can learn these skills when given appropriate training data and choice of\ngeneration strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models\nand code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn\ndialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing\nfailure cases of our models.*\n\nTips:\n\n- Blenderbot is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n\nThis model was contributed by [sshleifer](https://huggingface.co/sshleifer). The authors' code can be found [here](https://github.com/facebookresearch/ParlAI) .\n\n\n", "blenderbot-small": "## Overview\n\nThe Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\nJing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston on 30 Apr 2020.\n\nThe abstract of the paper is the following:\n\n*Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that\nscaling neural models in the number of parameters and the size of the data they are trained on gives improved results,\nwe show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of\nskills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to\ntheir partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent\npersona. We show that large scale models can learn these skills when given appropriate training data and choice of\ngeneration strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models\nand code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn\ndialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing\nfailure cases of our models.*\n\nTips:\n\n- Blenderbot Small is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The authors' code can be\nfound [here](https://github.com/facebookresearch/ParlAI).\n\n", "bloom": "## Overview\n\nThe BLOOM model has been proposed with its various versions through the [BigScience Workshop](https://bigscience.huggingface.co/). BigScience is inspired by other open science initiatives where researchers have pooled their time and resources to collectively achieve a higher impact.\nThe architecture of BLOOM is essentially similar to GPT3 (auto-regressive model for next token prediction), but has been trained on 46 different languages and 13 programming languages.\nSeveral smaller versions of the models have been trained on the same dataset. BLOOM is available in the following versions:\n\n- [bloom-560m](https://huggingface.co/bigscience/bloom-560m)\n- [bloom-1b1](https://huggingface.co/bigscience/bloom-1b1)\n- [bloom-1b7](https://huggingface.co/bigscience/bloom-1b7)\n- [bloom-3b](https://huggingface.co/bigscience/bloom-3b)\n- [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1)\n- [bloom](https://huggingface.co/bigscience/bloom) (176B parameters)\n\n", "camembert": "## Overview\n\nThe CamemBERT model was proposed in [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz Su\u00e1rez, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la\nClergerie, Djam\u00e9 Seddah, and Beno\u00eet Sagot. It is based on Facebook's RoBERTa model released in 2019. It is a model\ntrained on 138GB of French text.\n\nThe abstract from the paper is the following:\n\n*Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available\nmodels have either been trained on English data or on the concatenation of data in multiple languages. This makes\npractical use of such models --in all languages except English-- very limited. Aiming to address this issue for French,\nwe release CamemBERT, a French version of the Bi-directional Encoders for Transformers (BERT). We measure the\nperformance of CamemBERT compared to multilingual models in multiple downstream tasks, namely part-of-speech tagging,\ndependency parsing, named-entity recognition, and natural language inference. CamemBERT improves the state of the art\nfor most of the tasks considered. We release the pretrained model for CamemBERT hoping to foster research and\ndownstream applications for French NLP.*\n\nTips:\n\n- This implementation is the same as RoBERTa. Refer to the [documentation of RoBERTa](roberta) for usage examples\n  as well as the information relative to the inputs and outputs.\n\nThis model was contributed by [camembert](https://huggingface.co/camembert). The original code can be found [here](https://camembert-model.fr/).\n\n", "codegen": "## Overview\n\nThe CodeGen model was proposed in [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.\n\nCodeGen is an autoregressive language model for program synthesis trained sequentially on [The Pile](https://pile.eleuther.ai/), BigQuery, and BigPython.\n\nThe abstract from the paper is the following:\n\n*Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: [this https URL](https://github.com/salesforce/codegen).* \n\nThis model was contributed by [Hiroaki Hayashi](https://huggingface.co/rooa).\nThe original code can be found [here](https://github.com/salesforce/codegen).\n\n", "ctrl": "## Overview\n\nCTRL model was proposed in [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and\nRichard Socher. It's a causal (unidirectional) transformer pre-trained using language modeling on a very large corpus\nof ~140 GB of text data with the first token reserved as a control code (such as Links, Books, Wikipedia etc.).\n\nThe abstract from the paper is the following:\n\n*Large-scale language models show promising text generation capabilities, but users cannot easily control particular\naspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model,\ntrained to condition on control codes that govern style, content, and task-specific behavior. Control codes were\nderived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while\nproviding more explicit control over text generation. These codes also allow CTRL to predict which parts of the\ntraining data are most likely given a sequence. This provides a potential method for analyzing large amounts of data\nvia model-based source attribution.*\n\nTips:\n\n- CTRL makes use of control codes to generate text: it requires generations to be started by certain words, sentences\n  or links to generate coherent text. Refer to the [original implementation](https://github.com/salesforce/ctrl) for\n  more information.\n- CTRL is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n- CTRL was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\n  token in a sequence. Leveraging this feature allows CTRL to generate syntactically coherent text as it can be\n  observed in the *run_generation.py* example script.\n- The PyTorch models can take the `past_key_values` as input, which is the previously computed key/value attention pairs.\n  TensorFlow models accepts `past` as input. Using the `past_key_values` value prevents the model from re-computing\n  pre-computed values in the context of text generation. See the [`forward`](model_doc/ctrl#transformers.CTRLModel.forward)\n  method for more information on the usage of this argument.\n\nThis model was contributed by [keskarnitishr](https://huggingface.co/keskarnitishr). The original code can be found\n[here](https://github.com/salesforce/ctrl).\n\n", "deberta": "## Overview\n\nThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\nBERT model released in 2018 and Facebook's RoBERTa model released in 2019.\n\nIt builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\nRoBERTa.\n\nThe abstract from the paper is the following:\n\n*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to\npredict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency\nof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of\nthe training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%\n(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and\npre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.*\n\n\nThis model was contributed by [DeBERTa](https://huggingface.co/DeBERTa). This model TF 2.0 implementation was\ncontributed by [kamalkraj](https://huggingface.co/kamalkraj) . The original code can be found [here](https://github.com/microsoft/DeBERTa).\n\n", "deberta-v2": "## Overview\n\nThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\nBERT model released in 2018 and Facebook's RoBERTa model released in 2019.\n\nIt builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\nRoBERTa.\n\nThe abstract from the paper is the following:\n\n*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to\npredict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency\nof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of\nthe training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%\n(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and\npre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.*\n\n\nThe following information is visible directly on the [original implementation\nrepository](https://github.com/microsoft/DeBERTa). DeBERTa v2 is the second version of the DeBERTa model. It includes\nthe 1.5B model used for the SuperGLUE single-model submission and achieving 89.9, versus human baseline 89.8. You can\nfind more details about this submission in the authors'\n[blog](https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/)\n\nNew in v2:\n\n- **Vocabulary** In v2 the tokenizer is changed to use a new vocabulary of size 128K built from the training data.\n  Instead of a GPT2-based tokenizer, the tokenizer is now\n  [sentencepiece-based](https://github.com/google/sentencepiece) tokenizer.\n- **nGiE(nGram Induced Input Encoding)** The DeBERTa-v2 model uses an additional convolution layer aside with the first\n  transformer layer to better learn the local dependency of input tokens.\n- **Sharing position projection matrix with content projection matrix in attention layer** Based on previous\n  experiments, this can save parameters without affecting the performance.\n- **Apply bucket to encode relative positions** The DeBERTa-v2 model uses log bucket to encode relative positions\n  similar to T5.\n- **900M model & 1.5B model** Two additional model sizes are available: 900M and 1.5B, which significantly improves the\n  performance of downstream tasks.\n\nThis model was contributed by [DeBERTa](https://huggingface.co/DeBERTa). This model TF 2.0 implementation was\ncontributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/microsoft/DeBERTa).\n\n", "distilbert": "## Overview\n\nThe DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, a\ndistilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper [DistilBERT, a\ndistilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/papers/1910.01108). DistilBERT is a\nsmall, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than\n*bert-base-uncased*, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language\nunderstanding benchmark.\n\nThe abstract from the paper is the following:\n\n*As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),\noperating these large models in on-the-edge and/or under constrained computational training or inference budgets\nremains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation\nmodel, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger\ncounterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage\nknowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by\n40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive\nbiases learned by larger models during pretraining, we introduce a triple loss combining language modeling,\ndistillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we\ndemonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device\nstudy.*\n\nTips:\n\n- DistilBERT doesn't have `token_type_ids`, you don't need to indicate which token belongs to which segment. Just\n  separate your segments with the separation token `tokenizer.sep_token` (or `[SEP]`).\n- DistilBERT doesn't have options to select the input positions (`position_ids` input). This could be added if\n  necessary though, just let us know if you need this option.\n- Same as BERT but smaller. Trained by distillation of the pretrained BERT model, meaning it\u2019s been trained to predict the same probabilities as the larger model. The actual objective is a combination of:\n\n    * finding the same probabilities as the teacher model\n    * predicting the masked tokens correctly (but no next-sentence objective)\n    * a cosine similarity between the hidden states of the student and the teacher model\n\nThis model was contributed by [victorsanh](https://huggingface.co/victorsanh). This model jax version was\ncontributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation).\n\n", "electra": "## Overview\n\nThe ELECTRA model was proposed in the paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\nGenerators](https://openreview.net/pdf?id=r1xMH1BtvB). ELECTRA is a new pretraining approach which trains two\ntransformer models: the generator and the discriminator. The generator's role is to replace tokens in a sequence, and\nis therefore trained as a masked language model. The discriminator, which is the model we're interested in, tries to\nidentify which tokens were replaced by the generator in the sequence.\n\nThe abstract from the paper is the following:\n\n*Masked language modeling (MLM) pretraining methods such as BERT corrupt the input by replacing some tokens with [MASK]\nand then train a model to reconstruct the original tokens. While they produce good results when transferred to\ndownstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a\nmore sample-efficient pretraining task called replaced token detection. Instead of masking the input, our approach\ncorrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead\nof training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that\npredicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments\ndemonstrate this new pretraining task is more efficient than MLM because the task is defined over all input tokens\nrather than just the small subset that was masked out. As a result, the contextual representations learned by our\napproach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are\nparticularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained\nusing 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale,\nwhere it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when\nusing the same amount of compute.*\n\nTips:\n\n- ELECTRA is the pretraining approach, therefore there is nearly no changes done to the underlying model: BERT. The\n  only change is the separation of the embedding size and the hidden size: the embedding size is generally smaller,\n  while the hidden size is larger. An additional projection layer (linear) is used to project the embeddings from their\n  embedding size to the hidden size. In the case where the embedding size is the same as the hidden size, no projection\n  layer is used.\n- ELECTRA is a transformer model pretrained with the use of another (small) masked language model. The inputs are corrupted by that language model, which takes an input text that is randomly masked and outputs a text in which ELECTRA has to predict which token is an original and which one has been replaced. Like for GAN training, the small language model is trained for a few steps (but with the original texts as objective, not to fool the ELECTRA model like in a traditional GAN setting) then the ELECTRA model is trained for a few steps.\n- The ELECTRA checkpoints saved using [Google Research's implementation](https://github.com/google-research/electra)\n  contain both the generator and discriminator. The conversion script requires the user to name which model to export\n  into the correct architecture. Once converted to the HuggingFace format, these checkpoints may be loaded into all\n  available ELECTRA models, however. This means that the discriminator may be loaded in the\n  [`ElectraForMaskedLM`] model, and the generator may be loaded in the\n  [`ElectraForPreTraining`] model (the classification head will be randomly initialized as it\n  doesn't exist in the generator).\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). The original code can be found [here](https://github.com/google-research/electra).\n\n", "ernie": "## Overview\nERNIE is a series of powerful models proposed by baidu, especially in Chinese tasks,\nincluding [ERNIE1.0](https://arxiv.org/abs/1904.09223), [ERNIE2.0](https://ojs.aaai.org/index.php/AAAI/article/view/6428),\n[ERNIE3.0](https://arxiv.org/abs/2107.02137), [ERNIE-Gram](https://arxiv.org/abs/2010.12148), [ERNIE-health](https://arxiv.org/abs/2110.07244), etc.\n\nThese models are contributed by [nghuyong](https://huggingface.co/nghuyong) and the official code can be found in [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) (in PaddlePaddle).\n\n#", "flaubert": "## Overview\n\nThe FlauBERT model was proposed in the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le et al. It's a transformer model pretrained using a masked language\nmodeling (MLM) objective (like BERT).\n\nThe abstract from the paper is the following:\n\n*Language models have become a key step to achieve state-of-the art results in many different Natural Language\nProcessing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way\nto pre-train continuous word representations that can be fine-tuned for a downstream task, along with their\ncontextualization at the sentence level. This has been widely demonstrated for English using contextualized\nrepresentations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al.,\n2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and\nheterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for\nScientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text\nclassification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the\ntime they outperform other pretraining approaches. Different versions of FlauBERT as well as a unified evaluation\nprotocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research\ncommunity for further reproducible experiments in French NLP.*\n\nThis model was contributed by [formiel](https://huggingface.co/formiel). The original code can be found [here](https://github.com/getalp/Flaubert).\n\nTips:\n- Like RoBERTa, without the sentence ordering prediction (so just trained on the MLM objective).\n\n", "fnet": "## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\n*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.*\n\nTips on usage:\n\n- The model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\n  maximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\n  sequence length for fine-tuning and inference.\n\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f_net).\n\n", "git": "## Overview\n\nThe GIT model was proposed in [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang. GIT is a decoder-only Transformer\nthat leverages [CLIP](clip)'s vision encoder to condition the model on vision inputs besides text. The model obtains state-of-the-art results on\nimage captioning and visual question answering benchmarks.\n\nThe abstract from the paper is the following:\n\n*In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks.*\n\nTips:\n\n- GIT is implemented in a very similar way to GPT-2, the only difference being that the model is also conditioned on `pixel_values`.\n- One can use [`GitProcessor`] to prepare images for the model, and the `generate` method for autoregressive generation.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/git_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> GIT architecture. Taken from the <a href=\"https://arxiv.org/abs/2205.14100\" target=\"_blank\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/microsoft/GenerativeImage2Text).\n\n", "gpt-sw3": "## Overview\n\nThe GPT-Sw3 model was first proposed in\n[Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf)\nby Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey \u00d6hman,\nFredrik Carlsson, Magnus Sahlgren.\n\nSince that first paper the authors have extended their work and trained new models on their new 1.2TB corpora named The Nordic Pile.\n\nGPT-Sw3 is a collection of large decoder-only pretrained transformer language models that were developed by AI Sweden\nin collaboration with RISE and the WASP WARA for Media and Language. GPT-Sw3 has been trained on a dataset containing\n320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code. The model was pretrained using a\ncausal language modeling (CLM) objective utilizing the NeMo Megatron GPT implementation.\n\nThis model was contributed by [AI Sweden](https://huggingface.co/AI-Sweden).\n\nThe implementation uses the [GPT2Model](https://huggingface.co/docs/transformers/model_doc/gpt2) coupled\nwith our `GPTSw3Tokenizer`. This means that `AutoTokenizer` and `AutoModelForCausalLM` map to our tokenizer\nimplementation and the corresponding GPT2 model implementation respectively.\n*Note that sentencepiece is required to use our tokenizer and can be installed with:* `pip install transformers[sentencepiece]` or `pip install sentencepiece`\n\nExample usage:\n```python\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"AI-Sweden/gpt-sw3-356m\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"AI-Sweden/gpt-sw3-356m\")\n\n>>> input_ids = tokenizer(\"Tr\u00e4d \u00e4r fina f\u00f6r att\", return_tensors=\"pt\")[\"input_ids\"]\n\n>>> generated_token_ids = model.generate(inputs=input_ids, max_new_tokens=10, do_sample=True)[0]\n\n>>> print(tokenizer.decode(generated_token_ids))\nTr\u00e4d \u00e4r fina f\u00f6r att de \u00e4r f\u00e4rgstarka. Men ibland \u00e4r det fint\n```\n\n", "gpt2": "## Overview\n\nOpenAI GPT-2 model was proposed in [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) by Alec\nRadford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever from [OpenAI](https://huggingface.co/openai). It's a causal (unidirectional)\ntransformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n\nThe abstract from the paper is the following:\n\n*GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million\nweb pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some\ntext. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks\nacross diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than\n10X the amount of data.*\n\nTips:\n\n- GPT-2 is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n- GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\n  token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be\n  observed in the *run_generation.py* example script.\n- The model can take the *past_key_values* (for PyTorch) or *past* (for TF) as input, which is the previously computed\n  key/value attention pairs. Using this (*past_key_values* or *past*) value prevents the model from re-computing\n  pre-computed values in the context of text generation. For PyTorch, see *past_key_values* argument of the\n  [`GPT2Model.forward`] method, or for TF the *past* argument of the\n  [`TFGPT2Model.call`] method for more information on its usage.\n- Enabling the *scale_attn_by_inverse_layer_idx* and *reorder_and_upcast_attn* flags will apply the training stability\n  improvements from [Mistral](https://github.com/stanford-crfm/mistral/) (for PyTorch only).\n\n[Write With Transformer](https://transformer.huggingface.co/doc/gpt2-large) is a webapp created and hosted by\nHugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five\ndifferent sizes: small, medium, large, xl and a distilled version of the small checkpoint: *distilgpt-2*.\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://openai.com/blog/better-language-models/).\n\n", "gpt_bigcode": "## Overview\n\nThe GPTBigCode model was proposed in [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by BigCode. The listed authors are: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc\u00eda del R\u00edo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.\n\nThe abstract from the paper is the following:uery\n\n*The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at [this https URL.](https://huggingface.co/bigcode)*\n\nThe model is a an optimized [GPT2 model](https://huggingface.co/docs/transformers/model_doc/gpt2) with support for Multi-Query Attention.\n\n", "gpt_neox": "## Overview\n\nWe introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will\nbe made freely and openly available to the public through a permissive license. It is, to the best of our knowledge,\nthe largest dense autoregressive model that has publicly available weights at the time of submission. In this work,\nwe describe GPT-NeoX-20B's architecture and training and evaluate its performance on a range of language-understanding,\nmathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and\ngains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source\nthe training and evaluation code, as well as the model weights, at [https://github.com/EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox).\n\nDevelopment of the model was led by Sid Black, Stella Biderman and Eric Hallahan, and the model was trained with\ngenerous the support of [CoreWeave](https://www.coreweave.com/).\n\nGPT-NeoX-20B was trained with fp16, thus it is recommended to initialize the model as follows:\n\n```python\nmodel = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\").half().cuda()\n```\n\nGPT-NeoX-20B also has a different tokenizer from the one used in GPT-J-6B and GPT-Neo. The new tokenizer allocates\nadditional tokens to whitespace characters, making the model more suitable for certain tasks like code generation.\n\n#", "gpt_neox_japanese": "## Overview\n\nWe introduce GPT-NeoX-Japanese, which is an autoregressive language model for Japanese, trained on top of [https://github.com/EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox).\nJapanese is a unique language with its large vocabulary and a combination of hiragana, katakana, and kanji writing scripts.\nTo address this distinct structure of the Japanese language, we use a [special sub-word tokenizer](https://github.com/tanreinama/Japanese-BPEEncoder_V2). We are very grateful to *tanreinama* for open-sourcing this incredibly helpful tokenizer.\nFollowing the recommendations from Google's research on [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html), we have removed bias parameters from transformer blocks, achieving better model performance. Please refer [this article](https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4) in detail.\n\nDevelopment of the model was led by [Shinya Otani](https://github.com/SO0529), [Takayoshi Makabe](https://github.com/spider-man-tm), [Anuj Arora](https://github.com/Anuj040), and [Kyo Hattori](https://github.com/go5paopao) from [ABEJA, Inc.](https://www.abejainc.com/). For more information on this model-building activity, please refer [here (ja)](https://tech-blog.abeja.asia/entry/abeja-gpt-project-202207).\n\n#", "gptj": "## Overview\n\nThe GPT-J model was released in the [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repository by Ben Wang and Aran Komatsuzaki. It is a GPT-2-like\ncausal language model trained on [the Pile](https://pile.eleuther.ai/) dataset.\n\nThis model was contributed by [Stella Biderman](https://huggingface.co/stellaathena).\n\nTips:\n\n- To load [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) in float32 one would need at least 2x model size\n  RAM: 1x for initial weights and another 1x to load the checkpoint. So for GPT-J it would take at least 48GB\n  RAM to just load the model. To reduce the RAM usage there are a few options. The `torch_dtype` argument can be\n  used to initialize the model in half-precision on a CUDA device only. There is also a fp16 branch which stores the fp16 weights,\n  which could be used to further minimize the RAM usage:\n\n```python\n>>> from transformers import GPTJForCausalLM\n>>> import torch\n\n>>> device = \"cuda\"\n>>> model = GPTJForCausalLM.from_pretrained(\n...     \"EleutherAI/gpt-j-6B\",\n...     revision=\"float16\",\n...     torch_dtype=torch.float16,\n... ).to(device)\n```\n\n- The model should fit on 16GB GPU for inference. For training/fine-tuning it would take much more GPU RAM. Adam\n  optimizer for example makes four copies of the model: model, gradients, average and squared average of the gradients.\n  So it would need at least 4x model size GPU memory, even with mixed precision as gradient updates are in fp32. This\n  is not including the activations and data batches, which would again require some more GPU RAM. So one should explore\n  solutions such as DeepSpeed, to train/fine-tune the model. Another option is to use the original codebase to\n  train/fine-tune the model on TPU and then convert the model to Transformers format for inference. Instructions for\n  that could be found [here](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)\n\n- Although the embedding matrix has a size of 50400, only 50257 entries are used by the GPT-2 tokenizer. These extra\n  tokens are added for the sake of efficiency on TPUs. To avoid the mismatch between embedding matrix size and vocab\n  size, the tokenizer for [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) contains 143 extra tokens\n  `<|extratoken_1|>... <|extratoken_143|>`, so the `vocab_size` of tokenizer also becomes 50400.\n\n#", "ibert": "## Overview\n\nThe I-BERT model was proposed in [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney and Kurt Keutzer. It's a quantized version of RoBERTa running\ninference up to four times faster.\n\nThe abstract from the paper is the following:\n\n*Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language\nProcessing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive for\nefficient inference at the edge, and even at the data center. While quantization can be a viable solution for this,\nprevious work on quantizing Transformer based models use floating-point arithmetic during inference, which cannot\nefficiently utilize integer-only logical units such as the recent Turing Tensor Cores, or traditional integer-only ARM\nprocessors. In this work, we propose I-BERT, a novel quantization scheme for Transformer based models that quantizes\nthe entire inference with integer-only arithmetic. Based on lightweight integer-only approximation methods for\nnonlinear operations, e.g., GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end integer-only BERT\ninference without any floating point calculation. We evaluate our approach on GLUE downstream tasks using\nRoBERTa-Base/Large. We show that for both cases, I-BERT achieves similar (and slightly higher) accuracy as compared to\nthe full-precision baseline. Furthermore, our preliminary implementation of I-BERT shows a speedup of 2.4 - 4.0x for\nINT8 inference on a T4 GPU system as compared to FP32 inference. The framework has been developed in PyTorch and has\nbeen open-sourced.*\n\nThis model was contributed by [kssteven](https://huggingface.co/kssteven). The original code can be found [here](https://github.com/kssteven418/I-BERT).\n\n", "imagegpt": "## Overview\n\nThe ImageGPT model was proposed in [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt) by Mark\nChen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever. ImageGPT (iGPT) is a GPT-2-like\nmodel trained to predict the next pixel value, allowing for both unconditional and conditional image generation.\n\nThe abstract from the paper is the following:\n\n*Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models\ncan learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels,\nwithout incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels,\nwe find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and\nlow-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide\nResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also\ncompetitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0%\ntop-1 accuracy on a linear probe of our features.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/imagegpt_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Summary of the approach. Taken from the [original paper](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf). </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr), based on [this issue](https://github.com/openai/image-gpt/issues/7). The original code can be found\n[here](https://github.com/openai/image-gpt).\n\nTips:\n\n- ImageGPT is almost exactly the same as [GPT-2](gpt2), with the exception that a different activation\n  function is used (namely \"quick gelu\"), and the layer normalization layers don't mean center the inputs. ImageGPT\n  also doesn't have tied input- and output embeddings.\n- As the time- and memory requirements of the attention mechanism of Transformers scales quadratically in the sequence\n  length, the authors pre-trained ImageGPT on smaller input resolutions, such as 32x32 and 64x64. However, feeding a\n  sequence of 32x32x3=3072 tokens from 0..255 into a Transformer is still prohibitively large. Therefore, the authors\n  applied k-means clustering to the (R,G,B) pixel values with k=512. This way, we only have a 32*32 = 1024-long\n  sequence, but now of integers in the range 0..511. So we are shrinking the sequence length at the cost of a bigger\n  embedding matrix. In other words, the vocabulary size of ImageGPT is 512, + 1 for a special \"start of sentence\" (SOS)\n  token, used at the beginning of every sequence. One can use [`ImageGPTImageProcessor`] to prepare\n  images for the model.\n- Despite being pre-trained entirely unsupervised (i.e. without the use of any labels), ImageGPT produces fairly\n  performant image features useful for downstream tasks, such as image classification. The authors showed that the\n  features in the middle of the network are the most performant, and can be used as-is to train a linear model (such as\n  a sklearn logistic regression model for example). This is also referred to as \"linear probing\". Features can be\n  easily obtained by first forwarding the image through the model, then specifying `output_hidden_states=True`, and\n  then average-pool the hidden states at whatever layer you like.\n- Alternatively, one can further fine-tune the entire model on a downstream dataset, similar to BERT. For this, you can\n  use [`ImageGPTForImageClassification`].\n- ImageGPT comes in different sizes: there's ImageGPT-small, ImageGPT-medium and ImageGPT-large. The authors did also\n  train an XL variant, which they didn't release. The differences in size are summarized in the following table:\n\n| **Model variant** | **Depths** | **Hidden sizes** | **Decoder hidden size** | **Params (M)** | **ImageNet-1k Top 1** |\n|---|---|---|---|---|---|\n| MiT-b0 | [2, 2, 2, 2] | [32, 64, 160, 256] | 256 | 3.7 | 70.5 |\n| MiT-b1 | [2, 2, 2, 2] | [64, 128, 320, 512] | 256 | 14.0 | 78.7 |\n| MiT-b2 | [3, 4, 6, 3] | [64, 128, 320, 512] | 768 | 25.4 | 81.6 |\n| MiT-b3 | [3, 4, 18, 3] | [64, 128, 320, 512] | 768 | 45.2 | 83.1 |\n| MiT-b4 | [3, 8, 27, 3] | [64, 128, 320, 512] | 768 | 62.6 | 83.6 |\n| MiT-b5 | [3, 6, 40, 3] | [64, 128, 320, 512] | 768 | 82.0 | 83.8 |\n\n", "layoutlm": "## Overview\n\nThe LayoutLM model was proposed in the paper [LayoutLM: Pre-training of Text and Layout for Document Image\nUnderstanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and\nMing Zhou. It's a simple but effective pretraining method of text and layout for document image understanding and\ninformation extraction tasks, such as form understanding and receipt understanding. It obtains state-of-the-art results\non several downstream tasks:\n\n- form understanding: the [FUNSD](https://guillaumejaume.github.io/FUNSD/) dataset (a collection of 199 annotated\n  forms comprising more than 30,000 words).\n- receipt understanding: the [SROIE](https://rrc.cvc.uab.es/?ch=13) dataset (a collection of 626 receipts for\n  training and 347 receipts for testing).\n- document image classification: the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset (a collection of\n  400,000 images belonging to one of 16 classes).\n\nThe abstract from the paper is the following:\n\n*Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the\nwidespread use of pretraining models for NLP applications, they almost exclusively focus on text-level manipulation,\nwhile neglecting layout and style information that is vital for document image understanding. In this paper, we propose\nthe LayoutLM to jointly model interactions between text and layout information across scanned document images, which is\nbeneficial for a great number of real-world document image understanding tasks such as information extraction from\nscanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM.\nTo the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for\ndocument-level pretraining. It achieves new state-of-the-art results in several downstream tasks, including form\nunderstanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification\n(from 93.07 to 94.42).*\n\nTips:\n\n- In addition to *input_ids*, [`~transformers.LayoutLMModel.forward`] also expects the input `bbox`, which are\n  the bounding boxes (i.e. 2D-positions) of the input tokens. These can be obtained using an external OCR engine such\n  as Google's [Tesseract](https://github.com/tesseract-ocr/tesseract) (there's a [Python wrapper](https://pypi.org/project/pytesseract/) available). Each bounding box should be in (x0, y0, x1, y1) format, where\n  (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1, y1) represents the\n  position of the lower right corner. Note that one first needs to normalize the bounding boxes to be on a 0-1000\n  scale. To normalize, you can use the following function:\n\n```python\ndef normalize_bbox(bbox, width, height):\n    return [\n        int(1000 * (bbox[0] / width)),\n        int(1000 * (bbox[1] / height)),\n        int(1000 * (bbox[2] / width)),\n        int(1000 * (bbox[3] / height)),\n    ]\n```\n\nHere, `width` and `height` correspond to the width and height of the original document in which the token\noccurs. Those can be obtained using the Python Image Library (PIL) library for example, as follows:\n\n```python\nfrom PIL import Image\n\n# Document can be a png, jpg, etc. PDFs must be converted to images.\nimage = Image.open(name_of_your_document).convert(\"RGB\")\n\nwidth, height = image.size\n```\n\n", "led": "## Overview\n\nThe LED model was proposed in [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz\nBeltagy, Matthew E. Peters, Arman Cohan.\n\nThe abstract from the paper is the following:\n\n*Transformer-based models are unable to process long sequences due to their self-attention operation, which scales\nquadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or\nlonger. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we\nevaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our\npretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on\nWikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting\nlong document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization\ndataset.*\n\nTips:\n\n- [`LEDForConditionalGeneration`] is an extension of\n  [`BartForConditionalGeneration`] exchanging the traditional *self-attention* layer with\n  *Longformer*'s *chunked self-attention* layer. [`LEDTokenizer`] is an alias of\n  [`BartTokenizer`].\n- LED works very well on long-range *sequence-to-sequence* tasks where the `input_ids` largely exceed a length of\n  1024 tokens.\n- LED pads the `input_ids` to be a multiple of `config.attention_window` if required. Therefore a small speed-up is\n  gained, when [`LEDTokenizer`] is used with the `pad_to_multiple_of` argument.\n- LED makes use of *global attention* by means of the `global_attention_mask` (see\n  [`LongformerModel`]). For summarization, it is advised to put *global attention* only on the first\n  `<s>` token. For question answering, it is advised to put *global attention* on all tokens of the question.\n- To fine-tune LED on all 16384, *gradient checkpointing* can be enabled in case training leads to out-of-memory (OOM)\n  errors. This can be done by executing `model.gradient_checkpointing_enable()`. \n Moreover, the `use_cache=False`\n  flag can be used to disable the caching mechanism to save memory.\n- A notebook showing how to evaluate LED, can be accessed [here](https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing).\n- A notebook showing how to fine-tune LED, can be accessed [here](https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing).\n- LED is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n\n", "llama": "## Overview\n\nThe LLaMA model was proposed in [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. It is a collection of foundation language models ranging from 7B to 65B parameters.\n\nThe abstract from the paper is the following:\n\n*We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community. *\n\nTips:\n\n- Weights for the LLaMA models can be obtained from by filling out [this form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)\n- After downloading the weights, they will need to be converted to the Hugging Face Transformers format using the [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). The script can be called with the following (example) command:\n\n```bash\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\n```\n\n- After conversion, the model and tokenizer can be loaded via:\n\n```python\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained(\"/output/path\")\nmodel = LlamaForCausalLM.from_pretrained(\"/output/path\")\n```\n\nNote that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions\ncome in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM). For the 65B model, it's thus 130GB of RAM needed.\n\n- The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece). One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. \"Banana\"), the tokenizer does not prepend the prefix space to the string.\n\nThis model was contributed by [zphang](https://huggingface.co/zphang) with contributions from [BlackSamorez](https://huggingface.co/BlackSamorez). The code of the implementation in Hugging Face is based on GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox). The original code of the authors can be found [here](https://github.com/facebookresearch/llama).\n\n", "longformer": "## Overview\n\nThe Longformer model was presented in [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n\nThe abstract from the paper is the following:\n\n*Transformer-based models are unable to process long sequences due to their self-attention operation, which scales\nquadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or\nlonger. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we\nevaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our\npretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on\nWikiHop and TriviaQA.*\n\nTips:\n\n- Since the Longformer is based on RoBERTa, it doesn't have `token_type_ids`. You don't need to indicate which\n  token belongs to which segment. Just separate your segments with the separation token `tokenizer.sep_token` (or\n  `</s>`).\n- A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information.\n\nThis model was contributed by [beltagy](https://huggingface.co/beltagy). The Authors' code can be found [here](https://github.com/allenai/longformer).\n\n", "longt5": "## Overview\n\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention.\n\n\nThe abstract from the paper is the following:\n\n*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.*\n\nTips:\n\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\n- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (*transient* represents the fact the global tokens\nare constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\nThe complexity of this mechanism is `O(l(r + l/k))`.\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below.\n\n```python\n>>> import evaluate\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\n>>> dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n>>> model = (\n...     LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n...     .to(\"cuda\")\n...     .half()\n... )\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n\n\n>>> def generate_answers(batch):\n...     inputs_dict = tokenizer(\n...         batch[\"article\"], max_length=16384, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n...     )\n...     input_ids = inputs_dict.input_ids.to(\"cuda\")\n...     attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n...     output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=2)\n...     batch[\"predicted_abstract\"] = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n...     return batch\n\n\n>>> result = dataset.map(generate_answer, batched=True, batch_size=2)\n>>> rouge = evaluate.load(\"rouge\")\n>>> rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"])\n```\n\nThis model was contributed by [stancld](https://huggingface.co/stancld).\nThe original code can be found [here](https://github.com/google-research/longt5).\n\n", "luke": "## Overview\n\nThe LUKE model was proposed in [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda and Yuji Matsumoto.\nIt is based on RoBERTa and adds entity embeddings as well as an entity-aware self-attention mechanism, which helps\nimprove performance on various downstream tasks involving reasoning about entities such as named entity recognition,\nextractive and cloze-style question answering, entity typing, and relation classification.\n\nThe abstract from the paper is the following:\n\n*Entity representations are useful in natural language tasks involving entities. In this paper, we propose new\npretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed\nmodel treats words and entities in a given text as independent tokens, and outputs contextualized representations of\nthem. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves\npredicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also\npropose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the\ntransformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model\nachieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains\nstate-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question\nanswering).*\n\nTips:\n\n- This implementation is the same as [`RobertaModel`] with the addition of entity embeddings as well\n  as an entity-aware self-attention mechanism, which improves performance on tasks involving reasoning about entities.\n- LUKE treats entities as input tokens; therefore, it takes `entity_ids`, `entity_attention_mask`,\n  `entity_token_type_ids` and `entity_position_ids` as extra input. You can obtain those using\n  [`LukeTokenizer`].\n- [`LukeTokenizer`] takes `entities` and `entity_spans` (character-based start and end\n  positions of the entities in the input text) as extra input. `entities` typically consist of [MASK] entities or\n  Wikipedia entities. The brief description when inputting these entities are as follows:\n\n  - *Inputting [MASK] entities to compute entity representations*: The [MASK] entity is used to mask entities to be\n    predicted during pretraining. When LUKE receives the [MASK] entity, it tries to predict the original entity by\n    gathering the information about the entity from the input text. Therefore, the [MASK] entity can be used to address\n    downstream tasks requiring the information of entities in text such as entity typing, relation classification, and\n    named entity recognition.\n  - *Inputting Wikipedia entities to compute knowledge-enhanced token representations*: LUKE learns rich information\n    (or knowledge) about Wikipedia entities during pretraining and stores the information in its entity embedding. By\n    using Wikipedia entities as input tokens, LUKE outputs token representations enriched by the information stored in\n    the embeddings of these entities. This is particularly effective for tasks requiring real-world knowledge, such as\n    question answering.\n\n- There are three head models for the former use case:\n\n  - [`LukeForEntityClassification`], for tasks to classify a single entity in an input text such as\n    entity typing, e.g. the [Open Entity dataset](https://www.cs.utexas.edu/~eunsol/html_pages/open_entity.html).\n    This model places a linear head on top of the output entity representation.\n  - [`LukeForEntityPairClassification`], for tasks to classify the relationship between two entities\n    such as relation classification, e.g. the [TACRED dataset](https://nlp.stanford.edu/projects/tacred/). This\n    model places a linear head on top of the concatenated output representation of the pair of given entities.\n  - [`LukeForEntitySpanClassification`], for tasks to classify the sequence of entity spans, such as\n    named entity recognition (NER). This model places a linear head on top of the output entity representations. You\n    can address NER using this model by inputting all possible entity spans in the text to the model.\n\n  [`LukeTokenizer`] has a `task` argument, which enables you to easily create an input to these\n  head models by specifying `task=\"entity_classification\"`, `task=\"entity_pair_classification\"`, or\n  `task=\"entity_span_classification\"`. Please refer to the example code of each head models.\n\n  A demo notebook on how to fine-tune [`LukeForEntityPairClassification`] for relation\n  classification can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LUKE).\n\n  There are also 3 notebooks available, which showcase how you can reproduce the results as reported in the paper with\n  the HuggingFace implementation of LUKE. They can be found [here](https://github.com/studio-ousia/luke/tree/master/notebooks).\n\nExample:\n\n```python\n>>> from transformers import LukeTokenizer, LukeModel, LukeForEntityPairClassification\n\n>>> model = LukeModel.from_pretrained(\"studio-ousia/luke-base\")\n>>> tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n# Example 1: Computing the contextualized entity representation corresponding to the entity mention \"Beyonc\u00e9\"\n\n>>> text = \"Beyonc\u00e9 lives in Los Angeles.\"\n>>> entity_spans = [(0, 7)]  # character-based entity span corresponding to \"Beyonc\u00e9\"\n>>> inputs = tokenizer(text, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> word_last_hidden_state = outputs.last_hidden_state\n>>> entity_last_hidden_state = outputs.entity_last_hidden_state\n# Example 2: Inputting Wikipedia entities to obtain enriched contextualized representations\n\n>>> entities = [\n...     \"Beyonc\u00e9\",\n...     \"Los Angeles\",\n... ]  # Wikipedia entity titles corresponding to the entity mentions \"Beyonc\u00e9\" and \"Los Angeles\"\n>>> entity_spans = [(0, 7), (17, 28)]  # character-based entity spans corresponding to \"Beyonc\u00e9\" and \"Los Angeles\"\n>>> inputs = tokenizer(text, entities=entities, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> word_last_hidden_state = outputs.last_hidden_state\n>>> entity_last_hidden_state = outputs.entity_last_hidden_state\n# Example 3: Classifying the relationship between two entities using LukeForEntityPairClassification head model\n\n>>> model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n>>> tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n>>> entity_spans = [(0, 7), (17, 28)]  # character-based entity spans corresponding to \"Beyonc\u00e9\" and \"Los Angeles\"\n>>> inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n>>> predicted_class_idx = int(logits[0].argmax())\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nThis model was contributed by [ikuyamada](https://huggingface.co/ikuyamada) and [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/studio-ousia/luke).\n\n", "m2m_100": "## Overview\n\nThe M2M100 model was proposed in [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,\nSiddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy\nLiptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n\nThe abstract from the paper is the following:\n\n*Existing work in translation demonstrated the potential of massively multilingual machine translation by training a\nsingle model able to translate between any pair of languages. However, much of this work is English-Centric by training\nonly on data which was translated from or to English. While this is supported by large sources of training data, it\ndoes not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation\nmodel that can translate directly between any pair of 100 languages. We build and open source a training dataset that\ncovers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how\nto effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters\nto create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly\ntranslating between non-English directions while performing competitively to the best single systems of WMT. We\nopen-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model.*\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla).\n\n\n#", "markuplm": "## Overview\n\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm).\n\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\n\nThe abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.*\n\nTips:\n- In addition to `input_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`.\nThese are the XPATH tags and subscripts respectively for each token in the input sequence.\n- One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.\n- Demo notebooks can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM).\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> MarkupLM architecture. Taken from the <a href=\"https://arxiv.org/abs/2110.08518\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).\n\n", "mbart": "## Overview of MBart\n\nThe MBart model was presented in [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov Marjan\nGhazvininejad, Mike Lewis, Luke Zettlemoyer.\n\nAccording to the abstract, MBART is a sequence-to-sequence denoising auto-encoder pretrained on large-scale monolingual\ncorpora in many languages using the BART objective. mBART is one of the first methods for pretraining a complete\nsequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only\non the encoder, decoder, or reconstructing parts of the text.\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla). The Authors' code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/mbart)\n\n#", "mega": "## Overview\n\nThe MEGA model was proposed in [Mega: Moving Average Equipped Gated Attention](https://arxiv.org/abs/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\nMEGA proposes a new approach to self-attention with each encoder layer having a multi-headed exponential moving average in addition to a single head of standard dot-product attention, giving the attention mechanism \nstronger positional biases. This allows MEGA to perform competitively to Transformers on standard benchmarks including LRA \nwhile also having significantly fewer parameters. MEGA's compute efficiency allows it to scale to very long sequences, making it an \nattractive option for long-document NLP tasks.\n\nThe abstract from the paper is the following:\n\n *The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models. *\n\nTips:\n\n- MEGA can perform quite well with relatively few parameters. See Appendix D in the MEGA paper for examples of architectural specs which perform well in various settings. If using MEGA as a decoder, be sure to set `bidirectional=False` to avoid errors with default bidirectional. \n- Mega-chunk is a variant of mega that reduces time and spaces complexity from quadratic to linear. Utilize chunking with MegaConfig.use_chunking and control chunk size with MegaConfig.chunk_size \n\nThis model was contributed by [mnaylor](https://huggingface.co/mnaylor).\nThe original code can be found [here](https://github.com/facebookresearch/mega).\n\nImplementation Notes:\n\n- The original implementation of MEGA had an inconsistent expectation of attention masks for padding and causal self-attention between the softmax attention and Laplace/squared ReLU method. This implementation addresses that inconsistency.\n- The original implementation did not include token type embeddings; this implementation adds support for these, with the option controlled by MegaConfig.add_token_type_embeddings\n\n\n", "megatron-bert": "## Overview\n\nThe MegatronBERT model was proposed in [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model\nParallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper and Bryan Catanzaro.\n\nThe abstract from the paper is the following:\n\n*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in\nNatural Language Processing applications. However, very large models can be quite difficult to train due to memory\nconstraints. In this work, we present our techniques for training very large transformer models and implement a simple,\nefficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our\napproach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We\nillustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain\n15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline\nthat sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance\nthe state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9\nbillion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in\nBERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we\nachieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA\naccuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy\nof 89.4%).*\n\nTips:\n\nWe have provided pretrained [BERT-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m) checkpoints\nfor use to evaluate or finetuning downstream tasks.\n\nTo access these checkpoints, first [sign up](https://ngc.nvidia.com/signup) for and setup the NVIDIA GPU Cloud (NGC)\nRegistry CLI. Further documentation for downloading models can be found in the [NGC documentation](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).\n\nAlternatively, you can directly download the checkpoints using:\n\nBERT-345M-uncased:\n\n```bash\nwget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_uncased/zip\n-O megatron_bert_345m_v0_1_uncased.zip\n```\n\nBERT-345M-cased:\n\n```bash\nwget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_cased/zip -O\nmegatron_bert_345m_v0_1_cased.zip\n```\n\nOnce you have obtained the checkpoints from NVIDIA GPU Cloud (NGC), you have to convert them to a format that will\neasily be loaded by Hugging Face Transformers and our port of the BERT code.\n\nThe following commands allow you to do the conversion. We assume that the folder `models/megatron_bert` contains\n`megatron_bert_345m_v0_1_{cased, uncased}.zip` and that the commands are run from inside that folder:\n\n```bash\npython3 $PATH_TO_TRANSFORMERS/models/megatron_bert/convert_megatron_bert_checkpoint.py megatron_bert_345m_v0_1_uncased.zip\n```\n\n```bash\npython3 $PATH_TO_TRANSFORMERS/models/megatron_bert/convert_megatron_bert_checkpoint.py megatron_bert_345m_v0_1_cased.zip\n```\n\nThis model was contributed by [jdemouth](https://huggingface.co/jdemouth). The original code can be found [here](https://github.com/NVIDIA/Megatron-LM). That repository contains a multi-GPU and multi-node implementation of the\nMegatron Language models. In particular, it contains a hybrid model parallel approach using \"tensor parallel\" and\n\"pipeline parallel\" techniques.\n\n", "mobilebert": "## Overview\n\nThe MobileBERT model was proposed in [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny\nZhou. It's a bidirectional transformer based on the BERT model, which is compressed and accelerated using several\napproaches.\n\nThe abstract from the paper is the following:\n\n*Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds\nof millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot\nbe deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating\nthe popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to\nvarious downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while\nequipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks.\nTo train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE\nmodel. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is\n4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the\nnatural language inference tasks of GLUE, MobileBERT achieves a GLUEscore o 77.7 (0.6 lower than BERT_BASE), and 62 ms\nlatency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of\n90.0/79.2 (1.5/2.1 higher than BERT_BASE).*\n\nTips:\n\n- MobileBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n  than the left.\n- MobileBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n\nThis model was contributed by [vshampor](https://huggingface.co/vshampor). The original code can be found [here](https://github.com/google-research/mobilebert).\n\n", "mpnet": "## Overview\n\nThe MPNet model was proposed in [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n\nMPNet adopts a novel pre-training method, named masked and permuted language modeling, to inherit the advantages of\nmasked language modeling and permuted language modeling for natural language understanding.\n\nThe abstract from the paper is the following:\n\n*BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models.\nSince BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for\npre-training to address this problem. However, XLNet does not leverage the full position information of a sentence and\nthus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel\npre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the\ndependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position\ninformation as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in\nXLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of\ndown-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large\nmargin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g.,\nBERT, XLNet, RoBERTa) under the same model setting.*\n\nTips:\n\n- MPNet doesn't have `token_type_ids`, you don't need to indicate which token belongs to which segment. just\n  separate your segments with the separation token `tokenizer.sep_token` (or `[sep]`).\n\nThe original code can be found [here](https://github.com/microsoft/MPNet).\n\n", "mvp": "## Overview\n\nThe MVP model was proposed in [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n\n\nAccording to the abstract,\n\n- MVP follows a standard Transformer encoder-decoder architecture.\n- MVP is supervised pre-trained using labeled datasets.\n- MVP also has task-specific soft prompts to stimulate the model's capacity in performing a certain task.\n- MVP is specially designed for natural language generation and can be adapted to a wide range of generation tasks, including but not limited to summarization, data-to-text generation, open-ended dialogue system, story generation, question answering, question generation, task-oriented dialogue system, commonsense generation, paraphrase generation, text style transfer, and text simplification. Our model can also be adapted to natural language understanding tasks such as sequence classification and (extractive) question answering.\n\nTips:\n- We have released a series of models [here](https://huggingface.co/models?filter=mvp), including MVP, MVP with task-specific prompts, and multi-task pre-trained variants.\n- If you want to use a model without prompts (standard Transformer), you can load it through `MvpForConditionalGeneration.from_pretrained('RUCAIBox/mvp')`.\n- If you want to use a model with task-specific prompts, such as summarization, you can load it through `MvpForConditionalGeneration.from_pretrained('RUCAIBox/mvp-summarization')`.\n- Our model supports lightweight prompt tuning following [Prefix-tuning](https://arxiv.org/abs/2101.00190) with method `set_lightweight_tuning()`.\n\nThis model was contributed by [Tianyi Tang](https://huggingface.co/StevenTang). The detailed information and instructions can be found [here](https://github.com/RUCAIBox/MVP).\n\n", "nezha": "## Overview\n\nThe Nezha model was proposed in [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) by Junqiu Wei et al.\n\nThe abstract from the paper is the following:\n\n*The pre-trained language models have achieved great successes in various natural language understanding (NLU) tasks\ndue to its capacity to capture the deep contextualized information in text by pre-training on large-scale corpora.\nIn this technical report, we present our practice of pre-training language models named NEZHA (NEural contextualiZed\nrepresentation for CHinese lAnguage understanding) on Chinese corpora and finetuning for the Chinese NLU tasks. \nThe current version of NEZHA is based on BERT with a collection of proven improvements, which include Functional \nRelative Positional Encoding as an effective positional encoding scheme, Whole Word Masking strategy,\nMixed Precision Training and the LAMB Optimizer in training the models. The experimental results show that NEZHA\nachieves the state-of-the-art performances when finetuned on several representative Chinese tasks, including\nnamed entity recognition (People's Daily NER), sentence matching (LCQMC), Chinese sentiment classification (ChnSenti)\nand natural language inference (XNLI).*\n\nThis model was contributed by [sijunhe](https://huggingface.co/sijunhe). The original code can be found [here](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-PyTorch).\n\n", "nllb-moe": "## Overview\n\nThe NLLB model was presented in [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,\nLoic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,\nNecip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n\nThe abstract of the paper is the following:\n\n*Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today.\nHowever, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the\n200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by\nfirst contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed\nat narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of\nExperts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training\nimprovements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using\na human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.\nOur model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*\n\nTips:\n\n- M2M100ForConditionalGeneration is the base model for both NLLB and NLLB MoE\n- The NLLB-MoE is very similar to the NLLB model, but it's feed forward layer is based on the implementation of SwitchTransformers.\n- The tokenizer is the same as the NLLB models.\n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArtZucker).\nThe original code can be found [here](https://github.com/facebookresearch/fairseq).\n\n", "nystromformer": "## Overview\n\nThe Nystr\u00f6mformer model was proposed in [*Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\nFung, Yin Li, and Vikas Singh.\n\nThe abstract from the paper is the following:\n\n*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention\nwith O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.*\n\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer).\n\n", "open-llama": "## Overview\n\nThe Open-Llama model was proposed in [Open-Llama project](https://github.com/s-JoL/Open-Llama) by community developer s-JoL.\n\nThe model is mainly based on LLaMA with some modifications, incorporating memory-efficient attention from Xformers, stable embedding from Bloom, and shared input-output embedding from PaLM.\nAnd the model is pre-trained on both Chinese and English, which gives it better performance on Chinese language tasks.\n\nThis model was contributed by [s-JoL](https://huggingface.co/s-JoL).\nThe original code can be found [Open-Llama](https://github.com/s-JoL/Open-Llama).\nCheckpoint and usage can be found at [s-JoL/Open-Llama-V1](https://huggingface.co/s-JoL/Open-Llama-V1).\n\n\n", "openai-gpt": "## Overview\n\nOpenAI GPT model was proposed in [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\nby Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever. It's a causal (unidirectional) transformer\npre-trained using language modeling on a large corpus will long range dependencies, the Toronto Book Corpus.\n\nThe abstract from the paper is the following:\n\n*Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering,\nsemantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant,\nlabeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to\nperform adequately. We demonstrate that large gains on these tasks can be realized by generative pretraining of a\nlanguage model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In\ncontrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve\neffective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our\napproach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms\ndiscriminatively trained models that use architectures specifically crafted for each task, significantly improving upon\nthe state of the art in 9 out of the 12 tasks studied.*\n\nTips:\n\n- GPT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n- GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\n  token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be\n  observed in the *run_generation.py* example script.\n\n[Write With Transformer](https://transformer.huggingface.co/doc/gpt) is a webapp created and hosted by Hugging Face\nshowcasing the generative capabilities of several models. GPT is one of them.\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/openai/finetune-transformer-lm).\n\nNote:\n\nIf you want to reproduce the original tokenization process of the *OpenAI GPT* paper, you will need to install `ftfy`\nand `SpaCy`:\n\n```bash\npip install spacy ftfy==4.4.3\npython -m spacy download en\n```\n\nIf you don't install `ftfy` and `SpaCy`, the [`OpenAIGPTTokenizer`] will default to tokenize\nusing BERT's `BasicTokenizer` followed by Byte-Pair Encoding (which should be fine for most usage, don't worry).\n\n", "opt": "## Overview\n\nThe OPT model was proposed in [Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068) by Meta AI.\nOPT is a series of open-sourced large causal language models which perform similar in performance to GPT3.\n\nThe abstract from the paper is the following:\n\n*Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.*\n\nTips:\n- OPT has the same architecture as [`BartDecoder`].\n- Contrary to GPT2, OPT adds the EOS token `</s>` to the beginning of every prompt.\n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ), [Younes Belkada](https://huggingface.co/ybelkada), and [Patrick Von Platen](https://huggingface.co/patrickvonplaten).\nThe original code can be found [here](https://github.com/facebookresearch/metaseq).\n\n", "pegasus": "## Overview\n\nThe Pegasus model was proposed in [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/pdf/1912.08777.pdf) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019.\n\nAccording to the abstract,\n\n- Pegasus' pretraining task is intentionally similar to summarization: important sentences are removed/masked from an\n  input document and are generated together as one output sequence from the remaining sentences, similar to an\n  extractive summary.\n- Pegasus achieves SOTA summarization performance on all 12 downstream tasks, as measured by ROUGE and human eval.\n\nThis model was contributed by [sshleifer](https://huggingface.co/sshleifer). The Authors' code can be found [here](https://github.com/google-research/pegasus).\n\nTips:\n\n- Sequence-to-sequence model with the same encoder-decoder model architecture as BART. Pegasus is pre-trained jointly on two self-supervised objective functions: Masked Language Modeling (MLM) and a novel summarization specific pretraining objective, called Gap Sentence Generation (GSG).\n\n  * MLM: encoder input tokens are randomly replaced by a mask tokens and have to be predicted by the encoder (like in BERT)\n  * GSG: whole encoder input sentences are replaced by a second mask token and fed to the decoder, but which has a causal mask to hide the future words like a regular auto-regressive transformer decoder.\n\n", "pegasus_x": "## Overview\n\nThe PEGASUS-X model was proposed in [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347)  by Jason Phang, Yao Zhao and Peter J. Liu.\n\nPEGASUS-X (PEGASUS eXtended) extends the PEGASUS models for long input summarization through additional long input pretraining and using staggered block-local attention with global tokens in the encoder.\n\nThe abstract from the paper is the following:\n\n*While large pretrained Transformer models have proven highly capable at tackling natural language tasks, handling long sequence inputs continues to be a significant challenge. One such task is long input summarization, where inputs are longer than the maximum input context of most pretrained models. Through an extensive set of experiments, we investigate what model architectural changes and pretraining paradigms can most efficiently adapt a pretrained Transformer for long input summarization. We find that a staggered, block-local Transformer with global encoder tokens strikes a good balance of performance and efficiency, and that an additional pretraining phase on long sequences meaningfully improves downstream summarization performance. Based on our findings, we introduce PEGASUS-X, an extension of the PEGASUS model with additional long input pretraining to handle inputs of up to 16K tokens. PEGASUS-X achieves strong performance on long input summarization tasks comparable with much larger models while adding few additional parameters and not requiring model parallelism to train.*\n\nTips:\n\n* PEGASUS-X uses the same tokenizer as PEGASUS.\n\nThis model was contributed by [zphang](<https://huggingface.co/zphang). The original code can be found [here](https://github.com/google-research/pegasus).\n\n", "plbart": "## Overview of PLBart\n\nThe PLBART model was proposed in [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\nThis is a BART-like model which can be used to perform code-summarization, code-generation, and code-translation tasks. The pre-trained model `plbart-base` has been trained using multilingual denoising task\non Java, Python and English.\n\nAccording to the abstract\n\n*Code summarization and generation empower conversion between programming language (PL) and natural language (NL),\nwhile code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, \na sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks.\nPLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding.\nExperiments on code summarization in the English language, code generation, and code translation in seven programming languages\nshow that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program\nrepair, clone detection, and vulnerable code detection, demonstrate PLBART's effectiveness in program understanding.\nFurthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow\n(e.g., if block inside an else block is equivalent to else if block) that are crucial to program semantics and thus excels\neven with limited annotations.*\n\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The Authors' code can be found [here](https://github.com/wasiahmad/PLBART).\n\n#", "prophetnet": "## Overview\n\nThe ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\nZhang, Ming Zhou on 13 Jan, 2020.\n\nProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling instead of just\nthe next token.\n\nThe abstract from the paper is the following:\n\n*In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel\nself-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of\nthe optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by\nn-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time\nstep. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent\noverfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale\ndataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for\nabstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new\nstate-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.*\n\nTips:\n\n- ProphetNet is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n- The model architecture is based on the original Transformer, but replaces the \u201cstandard\u201d self-attention mechanism in the decoder by a a main self-attention mechanism and a self and n-stream (predict) self-attention mechanism.\n\nThe Authors' code can be found [here](https://github.com/microsoft/ProphetNet).\n\n", "reformer": "## Overview\n\nThe Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451.pdf) by Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya.\n\nThe abstract from the paper is the following:\n\n*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can\nbe prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\nTransformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\ncomplexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual\nlayers instead of the standard residuals, which allows storing activations only once in the training process instead of\nN times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models\nwhile being much more memory-efficient and much faster on long sequences.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The Authors' code can be\nfound [here](https://github.com/google/trax/tree/master/trax/models/reformer).\n\nTips:\n\n- Reformer does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035).\n- Use Axial position encoding (see below for more details). It\u2019s a mechanism to avoid having a huge positional encoding matrix (when the sequence length is very big) by factorizing it into smaller matrices.\n- Replace traditional attention by LSH (local-sensitive hashing) attention (see below for more details). It\u2019s a technique to avoid computing the full product query-key in the attention layers.\n- Avoid storing the intermediate results of each layer by using reversible transformer layers to obtain them during the backward pass (subtracting the residuals from the input of the next layer gives them back) or recomputing them for results inside a given layer (less efficient than storing them but saves memory).\n- Compute the feedforward operations by chunks and not on the whole batch.\n\n", "rembert": "## Overview\n\nThe RemBERT model was proposed in [Rethinking Embedding Coupling in Pre-trained Language Models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault F\u00e9vry, Henry Tsai, Melvin Johnson, Sebastian Ruder.\n\nThe abstract from the paper is the following:\n\n*We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art\npre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to\nsignificantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By\nreallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on\nstandard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that\nallocating additional capacity to the output embedding provides benefits to the model that persist through the\nfine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger\noutput embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage\nTransformer representations to be more general and more transferable to other tasks and languages. Harnessing these\nfindings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\nnumber of parameters at the fine-tuning stage.*\n\nTips:\n\nFor fine-tuning, RemBERT can be thought of as a bigger version of mBERT with an ALBERT-like factorization of the\nembedding layer. The embeddings are not tied in pre-training, in contrast with BERT, which enables smaller input\nembeddings (preserved during fine-tuning) and bigger output embeddings (discarded at fine-tuning). The tokenizer is\nalso similar to the Albert one rather than the BERT one.\n\n", "roberta": "## Overview\n\nThe RoBERTa model was proposed in [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, [Myle Ott](https://huggingface.co/myleott), Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google's BERT model released in 2018.\n\nIt builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with\nmuch larger mini-batches and learning rates.\n\nThe abstract from the paper is the following:\n\n*Language model pretraining has led to significant performance gains but careful comparison between different\napproaches is challenging. Training is computationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the final results. We present a replication\nstudy of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every\nmodel published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise questions about the source of recently\nreported improvements. We release our models and code.*\n\nTips:\n\n- This implementation is the same as [`BertModel`] with a tiny embeddings tweak as well as a setup\n  for Roberta pretrained models.\n- RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a\n  different pretraining scheme.\n- RoBERTa doesn't have `token_type_ids`, you don't need to indicate which token belongs to which segment. Just\n  separate your segments with the separation token `tokenizer.sep_token` (or `</s>`)\n- Same as BERT with better pretraining tricks:\n\n    * dynamic masking: tokens are masked differently at each epoch, whereas BERT does it once and for all\n    * together to reach 512 tokens (so the sentences are in an order than may span several documents)\n    * train with larger batches\n    * use BPE with bytes as a subunit and not characters (because of unicode characters)\n- [CamemBERT](camembert) is a wrapper around RoBERTa. Refer to this page for usage examples.\n\nThis model was contributed by [julien-c](https://huggingface.co/julien-c). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/roberta).\n\n", "roberta-prelayernorm": "## Overview\n\nThe RoBERTa-PreLayerNorm model was proposed in [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.\nIt is identical to using the `--encoder-normalize-before` flag in [fairseq](https://fairseq.readthedocs.io/).\n\nThe abstract from the paper is the following:\n\n*fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs.*\n\nTips:\n\n- The implementation is the same as [Roberta](roberta) except instead of using _Add and Norm_ it does _Norm and Add_. _Add_ and _Norm_ refers to the Addition and LayerNormalization as described in [Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n- This is identical to using the `--encoder-normalize-before` flag in [fairseq](https://fairseq.readthedocs.io/).\n\nThis model was contributed by [andreasmaden](https://huggingface.co/andreasmaden).\nThe original code can be found [here](https://github.com/princeton-nlp/DinkyTrain).\n\n", "roc_bert": "## Overview\n\nThe RoCBert model was proposed in [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf)  by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.\nIt's a pretrained Chinese language model that is robust under various forms of adversarial attacks.\n\nThe abstract from the paper is the following:\n\n*Large-scale pretrained language models have achieved SOTA results on NLP tasks. However, they have been shown\nvulnerable to adversarial attacks especially for logographic languages like Chinese. In this work, we propose\nROCBERT: a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation,\nsynonyms, typos, etc. It is pretrained with the contrastive learning objective which maximizes the label consistency\nunder different synthesized adversarial examples. The model takes as input multimodal information including the\nsemantic, phonetic and visual features. We show all these features are important to the model robustness since the\nattack can be performed in all the three forms. Across 5 Chinese NLU tasks, ROCBERT outperforms strong baselines under\nthree blackbox adversarial algorithms without sacrificing the performance on clean testset. It also performs the best\nin the toxic content detection task under human-made attacks.*\n\nThis model was contributed by [weiweishi](https://huggingface.co/weiweishi).\n\n", "roformer": "## Overview\n\nThe RoFormer model was proposed in [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864v1.pdf) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\n\nThe abstract from the paper is the following:\n\n*Position encoding in transformer architecture provides supervision for dependency modeling between elements at\ndifferent positions in the sequence. We investigate various methods to encode positional information in\ntransformer-based language models and propose a novel implementation named Rotary Position Embedding(RoPE). The\nproposed RoPE encodes absolute positional information with rotation matrix and naturally incorporates explicit relative\nposition dependency in self-attention formulation. Notably, RoPE comes with valuable properties such as flexibility of\nbeing expand to any sequence lengths, decaying inter-token dependency with increasing relative distances, and\ncapability of equipping the linear self-attention with relative position encoding. As a result, the enhanced\ntransformer with rotary position embedding, or RoFormer, achieves superior performance in tasks with long texts. We\nrelease the theoretical analysis along with some preliminary experiment results on Chinese data. The undergoing\nexperiment for English benchmark will soon be updated.*\n\nTips:\n\n- RoFormer is a BERT-like autoencoding model with rotary position embeddings. Rotary position embeddings have shown\n  improved performance on classification tasks with long texts.\n\n\nThis model was contributed by [junnyu](https://huggingface.co/junnyu). The original code can be found [here](https://github.com/ZhuiyiTechnology/roformer).\n\n", "rwkv": "## Overview\n\nThe RWKV model was proposed in [this repo](https://github.com/BlinkDL/RWKV-LM)\n\nIt suggests a tweak in the traditional Transformer attention to make it linear. This way, the model can be used as recurrent network: passing inputs for timestamp 0 and timestamp 1 together is the same as passing inputs at timestamp 0, then inputs at timestamp 1 along with the state of timestamp 0 (see example below).\n\nThis can be more efficient than a regular Transformer and can deal with sentence of any length (even if the model uses a fixed context length for training).\n\nThis model was contributed by [sgugger](https://huggingface.co/sgugger).\nThe original code can be found [here](https://github.com/BlinkDL/RWKV-LM).\n\nExample of use as an RNN:\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, RwkvConfig, RwkvModel\n\nmodel = RwkvModel.from_pretrained(\"sgugger/rwkv-430M-pile\")\ntokenizer = AutoTokenizer.from_pretrained(\"sgugger/rwkv-430M-pile\")\n\ninputs = tokenizer(\"This is an example.\", return_tensors=\"pt\")\n# Feed everything to the model\noutputs = model(inputs[\"input_ids\"])\noutput_whole = outputs.last_hidden_state\n\noutputs = model(inputs[\"input_ids\"][:, :2])\noutput_one = outputs.last_hidden_state\n\n# Using the state computed on the first inputs, we will get the same output\noutputs = model(inputs[\"input_ids\"][:, 2:], state=outputs.state)\noutput_two = outputs.last_hidden_state\n\ntorch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-5)\n```\n\n", "splinter": "## Overview\n\nThe Splinter model was proposed in [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy. Splinter\nis an encoder-only transformer (similar to BERT) pretrained using the recurring span selection task on a large corpus\ncomprising Wikipedia and the Toronto Book Corpus.\n\nThe abstract from the paper is the following:\n\nIn several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order\nof 100,000 annotated questions and answers. We explore the more realistic few-shot setting, where only a few hundred\ntraining examples are available, and observe that standard models perform poorly, highlighting the discrepancy between\ncurrent pretraining objectives and question answering. We propose a new pretraining scheme tailored for question\nanswering: recurring span selection. Given a passage with multiple sets of recurring spans, we mask in each set all\nrecurring spans but one, and ask the model to select the correct span in the passage for each masked span. Masked spans\nare replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select\nthe answer span. The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD\nwith only 128 training examples), while maintaining competitive performance in the high-resource setting.\n\nTips:\n\n- Splinter was trained to predict answers spans conditioned on a special [QUESTION] token. These tokens contextualize\n  to question representations which are used to predict the answers. This layer is called QASS, and is the default\n  behaviour in the [`SplinterForQuestionAnswering`] class. Therefore:\n- Use [`SplinterTokenizer`] (rather than [`BertTokenizer`]), as it already\n  contains this special token. Also, its default behavior is to use this token when two sequences are given (for\n  example, in the *run_qa.py* script).\n- If you plan on using Splinter outside *run_qa.py*, please keep in mind the question token - it might be important for\n  the success of your model, especially in a few-shot setting.\n- Please note there are two different checkpoints for each size of Splinter. Both are basically the same, except that\n  one also has the pretrained weights of the QASS layer (*tau/splinter-base-qass* and *tau/splinter-large-qass*) and one\n  doesn't (*tau/splinter-base* and *tau/splinter-large*). This is done to support randomly initializing this layer at\n  fine-tuning, as it is shown to yield better results for some cases in the paper.\n\nThis model was contributed by [yuvalkirstain](https://huggingface.co/yuvalkirstain) and [oriram](https://huggingface.co/oriram). The original code can be found [here](https://github.com/oriram/splinter).\n\n", "squeezebert": "## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:\n\n*Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets,\nlarge computing systems, and better neural network models, natural language processing (NLP) technology has made\nsignificant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant\nopportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we\nconsider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today's\nhighly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with\nBERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods\nsuch as grouped convolutions have yielded significant speedups for computer vision networks, but many of these\ntechniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*\n\nTips:\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n  rather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\n\nThis model was contributed by [forresti](https://huggingface.co/forresti).\n\n", "switch_transformers": "## Overview\n\nThe SwitchTransformers model was proposed in [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\n\nThe Switch Transformer model uses a sparse T5 encoder-decoder architecture, where the MLP are replaced by a Mixture of Experts (MoE). A routing mechanism (top 1 in this case) associates each token to one of the expert, where each expert is a dense MLP. While switch transformers have a lot more weights than their equivalent dense models, the sparsity allows better scaling and better finetuning performance at scale.\nDuring a forward pass, only a fraction of the weights are used. The routing mechanism allows the model to select relevant weights on the fly which increases the model capacity without increasing the number of operations.\n\n\nThe abstract from the paper is the following:\n\n*In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.*\n\nTips:\n\n- SwitchTransformers uses the [`T5Tokenizer`], which can be loaded directly from each model's repository.\n- The released weights are pretrained on English [Masked Language Modeling](https://moon-ci-docs.huggingface.co/docs/transformers/pr_19323/en/glossary#general-terms) task, and should be finetuned.\n\nThis model was contributed by [Younes Belkada](https://huggingface.co/ybelkada) and [Arthur Zucker](https://huggingface.co/ArtZucker) .\nThe original code can be found [here](https://github.com/google/flaxformer/tree/main/flaxformer/architectures/moe).\n\n", "t5": "## Overview\n\nThe T5 model was presented in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) by [Colin Raffel](https://huggingface.co/craffel), Noam Shazeer, [Adam Roberts](https://huggingface.co/adarob), Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, [Peter J. Liu](https://huggingface.co/peterjliu).\n\nThe abstract from the paper is the following:\n\n*Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream\ntask, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning\nhas given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of\ntransfer learning techniques for NLP by introducing a unified framework that converts every language problem into a\ntext-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled datasets, transfer\napproaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration\nwith scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering\nsummarization, question answering, text classification, and more. To facilitate future work on transfer learning for\nNLP, we release our dataset, pre-trained models, and code.*\n\nTips:\n\n- T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which\neach task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a\ndifferent prefix to the input corresponding to each task, e.g., for translation: *translate English to German: ...*,\nfor summarization: *summarize: ...*.\n- The pretraining includes both supervised and self-supervised training. Supervised training is conducted on downstream tasks provided by the GLUE and SuperGLUE benchmarks (converting them into text-to-text tasks as explained above).\n- Self-supervised training uses corrupted tokens, by randomly removing 15% of the tokens and replacing them with individual sentinel tokens (if several consecutive tokens are marked for removal, the whole group is replaced with a single sentinel token). The input of the encoder is the corrupted sentence, the input of the decoder is the original sentence and the target is then the dropped out tokens delimited by their sentinel tokens.\n\n- T5 uses relative scalar embeddings. Encoder input padding can be done on the left and on the right.\n\n- See the [training](#training), [inference](#inference) and [scripts](#scripts) sections below for all details regarding usage.\n\nT5 comes in different sizes:\n\n- [t5-small](https://huggingface.co/t5-small)\n\n- [t5-base](https://huggingface.co/t5-base)\n\n- [t5-large](https://huggingface.co/t5-large)\n\n- [t5-3b](https://huggingface.co/t5-3b)\n\n- [t5-11b](https://huggingface.co/t5-11b).\n\nBased on the original T5 model, Google has released some follow-up works:\n\n- **T5v1.1**: T5v1.1 is an improved version of T5 with some architectural tweaks, and is pre-trained on C4 only without\n  mixing in the supervised tasks. Refer to the documentation of T5v1.1 which can be found [here](t5v1.1).\n\n- **mT5**: mT5 is a multilingual T5 model. It is pre-trained on the mC4 corpus, which includes 101 languages. Refer to\n  the documentation of mT5 which can be found [here](mt5).\n\n- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer\n  to the documentation of byT5 which can be found [here](byt5).\n\n- **UL2**: UL2 is a T5 like model pretrained on various denoising objectives\n\n- **Flan-T5**: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the Flan collection of \n    datasets which include: `taskmaster2`, `djaym7/wiki_dialog`, `deepmind/code_contests`, `lambada`, `gsm8k`, `aqua_rat`, `esnli`, `quasc` and `qed`.\n\n- **FLan-UL2** : the UL2 model finetuned using the \"Flan\" prompt tuning and dataset collection.\n\n- **UMT5**: UmT5 is a multilingual T5 model trained on an improved and refreshed mC4 multilingual corpus,  29 trillion characters across 107 language, using a new sampling method, UniMax. Refer to\n the documentation of mT5 which can be found [here](umt5).\n\nAll checkpoints can be found on the [hub](https://huggingface.co/models?search=t5).\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/google-research/text-to-text-transfer-transformer).\n\n<a id='training'></a>\n\n", "transfo-xl": "## Overview\n\nThe Transformer-XL model was proposed in [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan\nSalakhutdinov. It's a causal (uni-directional) transformer with relative positioning (sinuso\u00efdal) embeddings which can\nreuse previously computed hidden-states to attend to longer context (memory). This model also uses adaptive softmax\ninputs and outputs (tied).\n\nThe abstract from the paper is the following:\n\n*Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the\nsetting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency\nbeyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a\nnovel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the\ncontext fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450%\nlonger than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+\ntimes faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of\nbpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn\nTreebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably\ncoherent, novel text articles with thousands of tokens.*\n\nTips:\n\n- Transformer-XL uses relative sinusoidal positional embeddings. Padding can be done on the left or on the right. The\n  original implementation trains on SQuAD with padding on the left, therefore the padding defaults are set to left.\n- Transformer-XL is one of the few models that has no sequence length limit.\n- Same as a regular GPT model, but introduces a recurrence mechanism for two consecutive segments (similar to a regular RNNs with two consecutive inputs). In this context, a segment is a number of consecutive tokens (for instance 512) that may span across multiple documents, and segments are fed in order to the model.\n- Basically, the hidden states of the previous segment are concatenated to the current input to compute the attention scores. This allows the model to pay attention to information that was in the previous segment as well as the current one. By stacking multiple attention layers, the receptive field can be increased to multiple previous segments.\n- This changes the positional embeddings to positional relative embeddings (as the regular positional embeddings would give the same results in the current input and the current hidden state at a given position) and needs to make some adjustments in the way attention scores are computed.\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/kimiyoung/transformer-xl).\n\n<Tip warning={true}>\n\nTransformerXL does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035)\n\n</Tip>\n\n", "visual_bert": "## Overview\n\nThe VisualBERT model was proposed in [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\nVisualBERT is a neural network trained on a variety of (image, text) pairs.\n\nThe abstract from the paper is the following:\n\n*We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks.\nVisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an\nassociated input image with self-attention. We further propose two visually-grounded language model objectives for\npre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2,\nand Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly\nsimpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any\nexplicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between\nverbs and image regions corresponding to their arguments.*\n\nTips:\n\n1. Most of the checkpoints provided work with the [`VisualBertForPreTraining`] configuration. Other\n   checkpoints provided are the fine-tuned checkpoints for down-stream tasks - VQA ('visualbert-vqa'), VCR\n   ('visualbert-vcr'), NLVR2 ('visualbert-nlvr2'). Hence, if you are not working on these downstream tasks, it is\n   recommended that you use the pretrained checkpoints.\n\n2. For the VCR task, the authors use a fine-tuned detector for generating visual embeddings, for all the checkpoints.\n   We do not provide the detector and its weights as a part of the package, but it will be available in the research\n   projects, and the states can be loaded directly into the detector provided.\n\n", "xglm": "## Overview\n\nThe XGLM model was proposed in [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668)\nby Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, \nShruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, \nJeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n\nThe abstract from the paper is the following:\n\n*Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language \ntasks without fine-tuning. While these models are known to be able to jointly represent many different languages, \ntheir training data is dominated by English, potentially limiting their cross-lingual generalization. \nIn this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, \nand study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters \nsets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size \nin multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) \nand natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, \nour model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the \nofficial supervised baseline in 45 directions. We present a detailed analysis of where the model succeeds and fails, \nshowing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement \non surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models \nin social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models.*\n\n\nThis model was contributed by [Suraj](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/xglm).\n\n", "xlm": "## Overview\n\nThe XLM model was proposed in [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by\nGuillaume Lample, Alexis Conneau. It's a transformer pretrained using one of the following objectives:\n\n- a causal language modeling (CLM) objective (next token prediction),\n- a masked language modeling (MLM) objective (BERT-like), or\n- a Translation Language Modeling (TLM) object (extension of BERT's MLM to multiple language inputs)\n\nThe abstract from the paper is the following:\n\n*Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding.\nIn this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We\npropose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual\ndata, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain\nstate-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our\napproach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we\nobtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised\nmachine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the\nprevious best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.*\n\nTips:\n\n- XLM has many different checkpoints, which were trained using different objectives: CLM, MLM or TLM. Make sure to\n  select the correct objective for your task (e.g. MLM checkpoints are not suitable for generation).\n- XLM has multilingual checkpoints which leverage a specific `lang` parameter. Check out the [multi-lingual](../multilingual) page for more information.\n- A transformer model trained on several languages. There are three different type of training for this model and the library provides checkpoints for all of them:\n\n    * Causal language modeling (CLM) which is the traditional autoregressive training (so this model could be in the previous section as well). One of the languages is selected for each training sample, and the model input is a sentence of 256 tokens, that may span over several documents in one of those languages.\n    * Masked language modeling (MLM) which is like RoBERTa. One of the languages is selected for each training sample, and the model input is a sentence of 256 tokens, that may span over several documents in one of those languages, with dynamic masking of the tokens.\n    * A combination of MLM and translation language modeling (TLM). This consists of concatenating a sentence in two different languages, with random masking. To predict one of the masked tokens, the model can use both, the surrounding context in language 1 and the context given by language 2.\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/facebookresearch/XLM/).\n\n", "xlm-prophetnet": "## Overview\n\nThe XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\nZhang, Ming Zhou on 13 Jan, 2020.\n\nXLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling instead of\njust the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual\n\"wiki100\" Wikipedia dump.\n\nThe abstract from the paper is the following:\n\n*In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel\nself-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of\nthe optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by\nn-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time\nstep. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent\noverfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale\ndataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for\nabstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new\nstate-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.*\n\nThe Authors' code can be found [here](https://github.com/microsoft/ProphetNet).\n\nTips:\n\n- XLM-ProphetNet's model architecture and pretraining objective is same as ProphetNet, but XLM-ProphetNet was pre-trained on the cross-lingual dataset XGLUE.\n\n", "xlm-roberta": "## Overview\n\nThe XLM-RoBERTa model was proposed in [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume\nWenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook's\nRoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl\ndata.\n\nThe abstract from the paper is the following:\n\n*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a\nwide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly\noutperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on\nXNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on\nlow-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We\nalso present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the\ntrade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource\nlanguages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing\nper-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We\nwill make XLM-R code, data, and models publicly available.*\n\nTips:\n\n- XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does\n  not require `lang` tensors to understand which language is used, and should be able to determine the correct\n  language from the input ids.\n- Uses RoBERTa tricks on the XLM approach, but does not use the translation language modeling objective. It only uses masked language modeling on sentences coming from one language.\n- This implementation is the same as RoBERTa. Refer to the [documentation of RoBERTa](roberta) for usage examples\n  as well as the information relative to the inputs and outputs.\n\nThis model was contributed by [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).\n\n", "xlm-roberta-xl": "## Overview\n\nThe XLM-RoBERTa-XL model was proposed in [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau. \n\nThe abstract from the paper is the following:\n\n*Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.*\n\nTips:\n\n- XLM-RoBERTa-XL is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does\n  not require `lang` tensors to understand which language is used, and should be able to determine the correct\n  language from the input ids.\n\nThis model was contributed by [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) and [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).\n\n", "xlnet": "## Overview\n\nThe XLNet model was proposed in [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\nQuoc V. Le. XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method to learn\nbidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization\norder.\n\nThe abstract from the paper is the following:\n\n*With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves\nbetter performance than pretraining approaches based on autoregressive language modeling. However, relying on\ncorrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a\npretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive\npretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all\npermutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive\nformulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into\npretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large\nmargin, including question answering, natural language inference, sentiment analysis, and document ranking.*\n\nTips:\n\n- The specific attention pattern can be controlled at training and test time using the `perm_mask` input.\n- Due to the difficulty of training a fully auto-regressive model over various factorization order, XLNet is pretrained\n  using only a sub-set of the output tokens as target which are selected with the `target_mapping` input.\n- To use XLNet for sequential decoding (i.e. not in fully bi-directional setting), use the `perm_mask` and\n  `target_mapping` inputs to control the attention span and outputs (see examples in\n  *examples/pytorch/text-generation/run_generation.py*)\n- XLNet is one of the few models that has no sequence length limit.\n- XLNet is not a traditional autoregressive model but uses a training strategy that builds on that. It permutes the tokens in the sentence, then allows the model to use the last n tokens to predict the token n+1. Since this is all done with a mask, the sentence is actually fed in the model in the right order, but instead of masking the first n tokens for n+1, XLNet uses a mask that hides the previous tokens in some given permutation of 1,\u2026,sequence length.\n- XLNet also uses the same recurrence mechanism as Transformer-XL to build long-term dependencies.\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/zihangdai/xlnet/).\n\n", "mra": "## Overview\n\nThe MRA model was proposed in [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://arxiv.org/abs/2207.10284) by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, and Vikas Singh.\n\nThe abstract from the paper is the following:\n\n*Transformers have emerged as a preferred model for many tasks in natural langugage processing and vision. Recent efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified sparsity patterns, low-rank basis expansions and combinations thereof. In this paper, we revisit classical Multiresolution Analysis (MRA) concepts such as Wavelets, whose potential value in this setting remains underexplored thus far. We show that simple approximations based on empirical feedback and design choices informed by modern hardware and implementation challenges, eventually yield a MRA-based approach for self-attention with an excellent performance profile across most criteria of interest. We undertake an extensive set of experiments and demonstrate that this multi-resolution scheme outperforms most efficient self-attention proposals and is favorable for both short and long sequences. Code is available at https://github.com/mlpen/mra-attention.*\n\nThis model was contributed by [novice03](https://huggingface.co/novice03).\nThe original code can be found [here](https://github.com/mlpen/mra-attention).\n\n\n", "mpt": "## Overview\n\nThe MPT model was proposed by the [MosaicML](https://www.mosaicml.com/) team and released with multiple sizes and finetuned variants. The MPT models is a series of open source and commercially usable LLMs pre-trained on 1T tokens. \n\nMPT models are GPT-style decoder-only transformers with several improvements: performance-optimized layer implementations, architecture changes that provide greater training stability, and the elimination of context length limits by replacing positional embeddings with ALiBi. \n\n- MPT base: MPT base pre-trained models on next token prediction \n- MPT instruct: MPT base models fine-tuned on instruction based tasks\n- MPT storywriter: MPT base models fine-tuned for 2500 steps on 65k-token excerpts of fiction books contained in the books3 corpus, this enables the model to handle very long sequences\n\nThe original code is available at the  [`llm-foundry`](https://github.com/mosaicml/llm-foundry/tree/main) repository.\n\nRead more about it [in the release blogpost](https://www.mosaicml.com/blog/mpt-7b)\n\nTips:\n\n- Learn more about some techniques behind training of the model [in this section of llm-foundry repository](https://github.com/mosaicml/llm-foundry/blob/main/TUTORIAL.md#faqs)\n- If you want to use the advanced version of the model (triton kernels, direct flash attention integration), you can still use the original model implementation by adding `trust_remote_code=True` when calling `from_pretrained`.\n\n- [Fine-tuning Notebook](https://colab.research.google.com/drive/1HCpQkLL7UXW8xJUJJ29X7QAeNJKO0frZ?usp=sharing) on how to fine-tune MPT-7B on a free Google Colab instance to turn the model into a Chatbot.\n\n\n", "code_llama": "## Overview\n\nThe Code Llama model was proposed in [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.\n\nThe abstract from the paper is the following:\n\n*We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.*\n\nCheck out all Code Llama models [here](https://huggingface.co/models?search=code_llama) and the officially released ones in the [codellama org](https://huggingface.co/codellama).\n\n<Tip warning={true}>\n\nThe `Llama2` family models, on which Code Llama is based, were trained using `bfloat16`, but the original inference uses `float16`. Let's look at the different precisions:\n\n* `float32`: PyTorch convention on model initialization is to load models in `float32`, no matter with which `dtype` the model weights were stored. `transformers` also follows this convention for consistency with PyTorch. This will be picked by default. If you want the `AutoModel` API to cast the load the checkpoints with the storage weights type, you must specify `torch_dtype=\"auto\"`, e.g. `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`.\n* `bfloat16`: Code Llama was trained with this precision, so we recommend using it for further training or fine-tuning.\n* `float16`: We recommend running inference using this precision, as it's usually faster than `bfloat16`, and evaluation metrics show no discernible degradation with respect to `bfloat16`. You can also run inference using `bfloat16`, and we recommend you check inference results with both `float16` and `bfloat16` after fine-tuning.\n\nAs mentioned above, the `dtype` of the storage weights is mostly irrelevant unless you are using `torch_dtype=\"auto\"` when initializing a model using. The reason is that the model will first be downloaded (using the `dtype` of the checkpoints online) and then will be casted to the default `dtype` of `torch` (becomes `torch.float32`). If there is a specified `torch_dtype`, it will be used instead.\n\n</Tip>\n\nTips:\n\n- These models have the same architecture as the `Llama2` models\n- The infilling task is supported out of the box. You should be using the `tokenizer.fill_token` where you want your input to be filled.\n- The model conversion script is the same as for the `Llama2` family:\n\nHere is a sample usage\n```bash\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\n```\nNote that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions\ncome in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM).\n\n- After conversion, the model and tokenizer can be loaded via:\n\n```python\n>>> from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n\n>>> tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n>>> model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n>>> PROMPT = '''def remove_non_ascii(s: str) -> str:\n    \"\"\" <FILL_ME>\n    return result\n'''\n>>> input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n>>> generated_ids = model.generate(input_ids, max_new_tokens=128)\n\n>>> filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n>>> print(PROMPT.replace(\"<FILL_ME>\", filling))\ndef remove_non_ascii(s: str) -> str:\n    \"\"\" Remove non-ASCII characters from a string.\n\n    Args:\n        s: The string to remove non-ASCII characters from.\n\n    Returns:\n        The string with non-ASCII characters removed.\n    \"\"\"\n    result = \"\"\n    for c in s:\n        if ord(c) < 128:\n            result += c\n    return result\n```\n\nIf you only want the infilled part:\n```python\n>>> from transformers import pipeline\n>>> import torch\n\n>>> generator = pipeline(\"text-generation\",model=\"codellama/CodeLlama-7b-hf\",torch_dtype=torch.float16, device_map=\"auto\")\n>>> generator('def remove_non_ascii(s: str) -> str:\\n    \"\"\" <FILL_ME>\\n    return result', max_new_tokens = 128, return_type = 1)\n```\n\nUnder the hood, the tokenizer [automatically splits by `<FILL_ME>`](https://huggingface.co/docs/transformers/main/model_doc/code_llama#transformers.CodeLlamaTokenizer.fill_token) to create a formatted input string that follows [the original training pattern](https://github.com/facebookresearch/codellama/blob/cb51c14ec761370ba2e2bc351374a79265d0465e/llama/generation.py#L402). This is more robust than preparing the pattern yourself: it avoids pitfalls, such as token glueing, that are very hard to debug.  To see how much CPU and GPU memory you need for this model or others, try [this calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) which can help determine that value.\n\n- The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece). One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. \"Banana\"), the tokenizer does not prepend the prefix space to the string.\n\nThis model was contributed by [ArthurZucker](https://huggingface.co/ArthurZ). The original code of the authors can be found [here](https://github.com/facebookresearch/llama).\n\n\n", "falcon": "## Overview\n\nFalcon is a class of causal decoder-only models built by [TII](https://www.tii.ae/). The largest Falcon checkpoints\nhave been trained on >=1T tokens of text, with a particular emphasis on the [RefinedWeb](https://arxiv.org/abs/2306.01116)\ncorpus. They are made available under the Apache 2.0 license.\n\n\nFalcon's architecture is modern and optimized for inference, with multi-query attention and support for efficient\nattention variants like `FlashAttention`. Both 'base' models trained only as causal language models as well as\n'instruct' models that have received further fine-tuning are available.\n\n\nFalcon models are (as of 2023) some of the largest and most powerful open-source language models,\nand consistently rank highly in the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n\n", "persimmon": "## Overview\n\nThe Persimmon model was created by [ADEPT](https://www.adept.ai/blog/persimmon-8b), and authored by Erich Elsen, Augustus Odena, Maxwell Nye, Sa\u011fnak Ta\u015f\u0131rlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.\n\nThe authors introduced Persimmon-8B, a decoder model based on the classic transformers architecture, with query and key normalization. Persimmon-8B is a fully permissively-licensed model with approximately 8 billion parameters, released under the Apache license.  Some of the key attributes of Persimmon-8B are long context size (16K), performance, and capabilities for multimodal extensions.\n\nThe authors showcase their approach to model evaluation, focusing on practical text generation, mirroring how users interact with language models. The work also includes a comparative analysis, pitting Persimmon-8B against other prominent models (MPT 7B Instruct and Llama 2 Base 7B 1-Shot), across various evaluation tasks. The results demonstrate Persimmon-8B's competitive performance, even with limited training data.\n\nIn terms of model details, the work outlines the architecture and training methodology of Persimmon-8B, providing insights into its design choices, sequence length, and dataset composition. The authors present a fast inference code that outperforms traditional implementations through operator fusion and CUDA graph utilization while maintaining code coherence. They express their anticipation of how the community will leverage this contribution to drive innovation, hinting at further upcoming releases as part of an ongoing series of developments.\n\n\n<Tip warning={true}>\n\nThe `Persimmon` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `torch_dtype = 'float16'` which will be\nused by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`. \n\nThe `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype` they want, and if they don't it will be `torch.float32`.\n\nFinetuning the model in `float16` is not recommended and known to produce `nan`, as such the model should be fine-tuned in `bfloat16`.\n\n</Tip>\n\n\nTips:\n\n- To convert the model, you need to clone the original repository using `git clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:\n\n```bash\ngit clone https://github.com/persimmon-ai-labs/adept-inference\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_base_model_release.tar\ntar -xvf 8b_base_model_release.tar\npython src/transformers/models/persimmon/convert_persimmon_weights_to_hf.py  --input_dir /path/to/downloaded/persimmon/weights/ --output_dir /output/path \\\n    --pt_model_path /path/to/8b_chat_model_release/iter_0001251/mp_rank_00/model_optim_rng.pt\n    --ada_lib_path /path/to/adept-inference\n```\n\nFor the chat model:\n```bash\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\ntar -xvf 8b_base_model_release.tar\n```\n\nThereafter, models can be loaded via:\n\n```py\nfrom transformers import PersimmonForCausalLM, PersimmonTokenizer\n\nmodel = PersimmonForCausalLM.from_pretrained(\"/output/path\")\ntokenizer = PersimmonTokenizer.from_pretrained(\"/output/path\")\n```\n\nThis model was contributed by [ArthurZ](https://huggingface.co/ArthurZ).\nThe original code can be found [here](https://github.com/persimmon-ai-labs/adept-inference).\n\n- Perismmon uses a `sentencepiece` based tokenizer, with a `Unigram` model. It supports bytefallback, which is only available in `tokenizers==0.14.0` for the fast tokenizer.\nThe `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece. The `chat` template will be updated with the templating functions in a follow up PR!\n\n- The authors suggest to use the following prompt format for the chat mode: `f\"human: {prompt}\\n\\nadept:\"`\n\n\n", "marian": "## Overview\n\nA framework for translation models, using the same models as BART. Translations should be similar, but not identical to output in the test set linked to in each model card.\nThis model was contributed by [sshleifer](https://huggingface.co/sshleifer).\n\n\n", "mixtral": "## Overview\n\nMixtral-8x7B is Mistral AI's second Large Language Model (LLM). \n\nThe Mixtral model was proposed by the [Mistral AI](https://mistral.ai/) team.\n\nIt was introduced in the [Mixtral of Experts blogpost](https://mistral.ai/news/mixtral-of-experts/) with the following introduction:\n\n*Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts models (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.*\n\nTips:\n\n\n- The model needs to be converted using the [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mixtral/convert_mixtral_weights_to_hf.py).\n- If the model is quantized to 4bits, a single A100 is enough to fit the entire 45B model.\n\nThis model was contributed by [Younes Belkada](https://huggingface.co/ybelkada) and [Arthur Zucker](https://huggingface.co/ArthurZ) .\nThe original code can be found [here](https://github.com/mistralai/mistral-src).\n\n\n#", "phi": "## Overview\n\nThe Phi-1 model was proposed in [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li.\n\nThe Phi-1.5 model was proposed in [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.\n\n#", "qwen2": "## Overview\n\nQwen2 is the new model series of large language models from the Qwen team. Previously, we released the Qwen series, including Qwen-72B, Qwen-1.8B, Qwen-VL, Qwen-Audio, etc.\n\n#", "cohere": "## Overview\n\nThe Cohere Command-R model was proposed in the blogpost [Command-R: Retrieval Augmented Generation at Production Scale](https://txt.cohere.com/command-r/) by the Cohere Team.\n\nThe abstract from the paper is the following:\n\n*Command-R is a scalable generative model targeting RAG and Tool Use to enable production-scale AI for enterprise. Today, we are introducing Command-R, a new LLM aimed at large-scale production workloads. Command-R targets the emerging \u201cscalable\u201d category of models that balance high efficiency with strong accuracy, enabling companies to move beyond proof of concept, and into production.*\n\n*Command-R is a generative model optimized for long context tasks such as retrieval augmented generation (RAG) and using external APIs and tools. It is designed to work in concert with our industry-leading Embed and Rerank models to provide best-in-class integration for RAG applications and excel at enterprise use cases. As a model built for companies to implement at scale, Command-R boasts:\n- Strong accuracy on RAG and Tool Use\n- Low latency, and high throughput\n- Longer 128k context and lower pricing\n- Strong capabilities across 10 key languages\n- Model weights available on HuggingFace for research and evaluation\n\nCheckout model checkpoints [here](https://huggingface.co/CohereForAI/c4ai-command-r-v01).\nThis model was contributed by [Saurabh Dash](https://huggingface.co/saurabhdash) and [Ahmet \u00dcst\u00fcn](https://huggingface.co/ahmetustun). The code of the implementation in Hugging Face is based on GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox).\n\n", "gemma": "## Overview\n\nThe Gemma model was proposed in [Gemma: Open Models Based on Gemini Technology and Research](https://blog.google/technology/developers/gemma-open-models/) by Gemma Team, Google.\nGemma models are trained on 6T tokens, and released with 2 versions, 2b and 7b.\n\nThe abstract from the paper is the following:\n\n*This work introduces Gemma, a new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of our model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations*\n\nTips:\n\n- The original checkpoints can be converted using the conversion script `src/transformers/models/gemma/convert_gemma_weights_to_hf.py` \n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ), [Younes Belkada](https://huggingface.co/ybelkada), [Sanchit Gandhi](https://huggingface.co/sanchit-gandhi), [Pedro Cuenca](https://huggingface.co/pcuenq).\n\n\n", "mamba": "## Overview\n\nThe Mamba model was proposed in [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) by Albert Gu and Tri Dao.\n\nThis model is a new paradigm architecture based on `state-space-models`. You can read more about the intuition behind these [here](https://srush.github.io/annotated-s4/).\n\nThe abstract from the paper is the following:\n\n*Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.*\n\nTips:\n\n- Mamba is a new `state space model` architecture that rivals the classic Transformers. It is based on the line of progress on structured state space models, with an efficient hardware-aware design and implementation in the spirit of [FlashAttention](https://github.com/Dao-AILab/flash-attention).\n- Mamba stacks `mixer` layers, which are the equivalent of `Attention` layers. The core logic of `mamba` is held in the `MambaMixer` class.\n- Two implementations cohabit: one is optimized and uses fast cuda kernels, while the other one is naive but can run on any device!\n- The current implementation leverages the original cuda kernels: the equivalent of flash attention for Mamba are hosted in the [`mamba-ssm`](https://github.com/state-spaces/mamba) and the [`causal_conv1d`](https://github.com/Dao-AILab/causal-conv1d) repositories. Make sure to install them if your hardware supports them!\n- Contributions to make the naive path faster are welcome \ud83e\udd17\n\nThis model was contributed by [ArthurZ](https://huggingface.co/ArthurZ).\nThe original code can be found [here](https://github.com/state-spaces/mamba).\n\n# Usage\n\n#", "stablelm": "## Overview\n\n`StableLM 3B 4E1T` was proposed in [`StableLM 3B 4E1T`: Technical Report](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo) by Stability AI and is the first model in a series of multi-epoch pre-trained language models.\n\n#", "starcoder2": "## Overview\n\nStarCoder2 is a family of open LLMs for code and comes in 3 different sizes with 3B, 7B and 15B parameters. The flagship StarCoder2-15B model is trained on over 4 trillion tokens and 600+ programming languages from The Stack v2. All models use Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and were trained using the Fill-in-the-Middle objective. The models have been released with the paper [StarCoder 2 and The Stack v2: The Next Generation](https://arxiv.org/abs/2402.19173) by Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau\u00df, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.\n\nThe abstract of the paper is the following:\n\n> The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.\n", "dbrx": "## Overview\n\nDBRX is a [transformer-based](https://www.isattentionallyouneed.com/) decoder-only large language model (LLM) that was trained using next-token prediction.\nIt uses a *fine-grained* mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input.\nIt was pre-trained on 12T tokens of text and code data.\nCompared to other open MoE models like Mixtral-8x7B and Grok-1, DBRX is fine-grained, meaning it uses a larger number of smaller experts. DBRX has 16 experts and chooses 4, while Mixtral-8x7B and Grok-1 have 8 experts and choose 2.\nThis provides 65x more possible combinations of experts and we found that this improves model quality.\nDBRX uses rotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA).\nIt is a BPE based model and uses the GPT-4 tokenizer as described in the [tiktoken](https://github.com/openai/tiktoken) repository.\nWe made these choices based on exhaustive evaluation and scaling experiments.\n\nDBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32K tokens.\nWe estimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of models.\nThis new dataset was developed using the full suite of Databricks tools, including Apache Spark\u2122 and Databricks notebooks for data processing, and Unity Catalog for data management and governance.\nWe used curriculum learning for pretraining, changing the data mix during training in ways we found to substantially improve model quality.\n\n\nMore detailed information about DBRX Instruct and DBRX Base can be found in our [technical blog post](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm).\n\nThis model was contributed by [eitan-turok](https://huggingface.co/eitanturok) and [abhi-db](https://huggingface.co/abhi-db). The original code can be found [here](https://github.com/databricks/dbrx-instruct), though this may not be up to date.\n\n", "olmo": "## Overview\n\nThe OLMo model was proposed in [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) by Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi.\n\nOLMo is a series of **O**pen **L**anguage **Mo**dels designed to enable the science of language models. The OLMo models are trained on the Dolma dataset. We release all code, checkpoints, logs (coming soon), and details involved in training these models.\n\nThe abstract from the paper is the following:\n\n*Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.*\n\nThis model was contributed by [shanearora](https://huggingface.co/shanearora).\nThe original code can be found [here](https://github.com/allenai/OLMo/tree/main/olmo).\n\n\n", "phi3": "## Overview\n\nThe Phi-3 model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) by Microsoft.\n\n#", "qwen2_moe": "## Overview\n\nQwen2MoE is the new model series of large language models from the Qwen team. Previously, we released the Qwen series, including Qwen-72B, Qwen-1.8B, Qwen-VL, Qwen-Audio, etc.\n\n#", "recurrent_gemma": "## Overview\n\nThe Recurrent Gemma model was proposed in [RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf) by the Griffin, RLHF and Gemma Teams of Google.\n\nThe abstract from the paper is the following:\n\n*We introduce RecurrentGemma, an open language model which uses Google\u2019s novel Griffin architecture. Griffin combines linear recurrences with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences. We provide a pre-trained model with 2B non-embedding parameters, and an instruction tuned variant. Both models achieve comparable performance to Gemma-2B despite being trained on fewer tokens.*\n\nTips:\n\n- The original checkpoints can be converted using the conversion script [`src/transformers/models/recurrent_gemma/convert_recurrent_gemma_weights_to_hf.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py). \n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/google-deepmind/recurrentgemma).\n\n\n", "gemma2": "## Overview\n\nThe Gemma2 model was proposed in [Gemma2: Open Models Based on Gemini Technology and Research](https://blog.google/technology/developers/google-gemma-2/) by Gemma2 Team, Google.\nTwo Gemma2 models are released, with parameters sizes of 9 billion (9B) and 27 billion (27B).\n\nThe abstract from the blog post is the following:\n\n*Now we\u2019re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December.*\n\nTips:\n\n- The original checkpoints can be converted using the conversion script `src/transformers/models/Gemma2/convert_Gemma2_weights_to_hf.py` \n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ), [Pedro Cuenca](https://huggingface.co/pcuenq) and [Tom Arsen]().\n\n\n", "falcon_mamba": "## Overview\n\nThe FalconMamba model was proposed by TII UAE (Technology Innovation Institute) in their release.\n\nThe abstract from the paper is the following:\n\n*We present FalconMamba, a new base large language model based on the novel Mamba architecture. FalconMamba is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, FalconMamba surpasses leading open-weight models based on Transformers, such as Mistral 7B, Llama3 8B, and Falcon2 11B. It is on par with Gemma 7B and outperforms models with different architecture designs, such as RecurrentGemma 9B. Currently, FalconMamba is the best-performing Mamba model in the literature at this scale, surpassing both existing Mamba and hybrid Mamba-Transformer models.\nDue to its architecture, FalconMamba is significantly faster at inference and requires substantially less memory for long sequence generation. Despite recent studies suggesting that hybrid Mamba-Transformer models outperform pure architecture designs, we argue and demonstrate that the pure Mamba design can achieve similar, even superior results compared to the hybrid design. We make the weights of our implementation of FalconMamba publicly available under a permissive license.*\n\nTips:\n\n- FalconMamba is mostly based on Mamba architecture, the same [tips and best practices](./mamba) would be relevant here.\n\nThe model has been trained on approximtely 6T tokens consisting a mixture of many data sources such as RefineWeb, Cosmopedia and Math data.\n\nFor more details about the training procedure and the architecture, have a look at [the technical paper of FalconMamba]() (coming soon).\n\n# Usage\n\nBelow we demonstrate how to use the model:\n\n```python \nfrom transformers import FalconMambaForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b\")\nmodel = FalconMambaForCausalLM.from_pretrained(\"tiiuae/falcon-mamba-7b\")\n\ninput_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n\nout = model.generate(input_ids, max_new_tokens=10)\nprint(tokenizer.batch_decode(out))\n```\n\nThe architecture is also compatible with `torch.compile` for faster generation:\n\n```python \nfrom transformers import FalconMambaForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b\")\nmodel = FalconMambaForCausalLM.from_pretrained(\"tiiuae/falcon-mamba-7b\", torch_dtype=torch.bfloat16).to(0)\nmodel = torch.compile(model)\n\ninput_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n\nout = model.generate(input_ids, max_new_tokens=10)\nprint(tokenizer.batch_decode(out))\n```\n\nIf you have access to a GPU that is compatible with `bitsandbytes`, you can also quantize the model in 4-bit precision:\n\n```python \nfrom transformers import FalconMambaForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b\")\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\nmodel = FalconMambaForCausalLM.from_pretrained(\"tiiuae/falcon-mamba-7b\", quantization_config=quantization_config)\n\ninput_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n\nout = model.generate(input_ids, max_new_tokens=10)\nprint(tokenizer.batch_decode(out))\n```\n\nYou can also play with the instruction fine-tuned model:\n\n```python \nfrom transformers import FalconMambaForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b-instruct\")\nmodel = FalconMambaForCausalLM.from_pretrained(\"tiiuae/falcon-mamba-7b-instruct\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True).input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n", "moshi": "## Overview\n\nThe Moshi model was proposed in [Moshi: a speech-text foundation model for real-time dialogue](https://kyutai.org/Moshi.pdf) by Alexandre D\u00e9fossez, Laurent Mazar\u00e9, Manu Orsini, Am\u00e9lie Royer, Patrick P\u00e9rez, Herv\u00e9 J\u00e9gou, Edouard Grave and Neil Zeghidour.\n\nMoshi is a speech-text foundation model that casts spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. Moshi also predicts time-aligned text tokens as a prefix to audio tokens. This \u201cInner Monologue\u201d method significantly improves the linguistic quality of generated speech and provides streaming speech recognition and text-to-speech. As a result, Moshi is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/ylacombe/benchmark-comparison/resolve/main/moshi_architecture.png\">\n</div>\n\nThe abstract from the paper is the following:\n\n*We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning\u2014 such as emotion or non-speech sounds\u2014 is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \u201cInner Monologue\u201d method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.* \n\nMoshi deals with 3 streams of information:\n1. The user's audio\n2. Moshi's audio\n3. Moshi's textual output\n\nSimilarly to [`~MusicgenModel`], audio is represented with audio codebooks, which can be interpreted like tokens. The main difference between text tokens and audio codebooks is that audio codebooks introduce an additional dimension of information.\nText tokens are typically of dim `(batch_size, sequence_length)` but audio tokens are of dim `(batch_size, num_codebooks, sequence_length)`.\n\nMoshi's made of 3 components:\n\n**1. The main decoder (Helium in the paper)**\n\nIt corresponds to [`MoshiForCausalLM`]. It is strictly a classic text LLM, that uses an architecture similar to [` ~GemmaForCausalLM`]. In other words, it takes text tokens, embeds them, pass them through the decoder and a language head, to get text logits.\n\n**2. The depth decoder**\n\nOn its own, it's also a classic LLM, but this time, instead of generating over the time dimension, it generates over the codebook dimension.\n\nIt also means that its context length is `num_codebooks`, thus it can't generate more than `num_codebooks`.\n\nNote that each timestamp - i.e each codebook - gets its own set of Linear Layers and Embeddings.\n\n**3. [`MimiModel`]**\n\nIt's the audio encoder from Kyutai, that has recently been integrated to transformers, which is used to \"tokenize\" audio. It has the same use that [`~EncodecModel`] has in [`~MusicgenModel`].\n\n\n", "olmoe": "## Overview\n\nThe OLMoE model was proposed in [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060) by Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi.\n\nOLMoE is a series of **O**pen **L**anguage **Mo**dels using sparse **M**ixture-**o**f-**E**xperts designed to enable the science of language models. We release all code, checkpoints, logs, and details involved in training these models.\n\nThe abstract from the paper is the following:\n\n*We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.*\n\nThis model was contributed by [Muennighoff](https://hf.co/Muennighoff).\nThe original code can be found [here](https://github.com/allenai/OLMoE).\n\n\n", "phimoe": "## Overview\n\nThe PhiMoE model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) by Microsoft.\n\n#", "cohere2": "## Overview\n[C4AI Command R7B](https://cohere.com/blog/command-r7b) is an open weights research release of a 7B billion parameter model developed by Cohere and Cohere For AI. It has advanced capabilities optimized for various use cases, including reasoning, summarization, question answering, and code. The model is trained to perform sophisticated tasks including Retrieval Augmented Generation (RAG) and tool use. The model also has powerful agentic capabilities that can use and combine multiple tools over multiple steps to accomplish more difficult tasks. It obtains top performance on enterprise-relevant code use cases. C4AI Command R7B is a multilingual model trained on 23 languages.\n\nThe model features three layers with sliding window attention (window size 4096) and ROPE for efficient local context modeling and relative positional encoding. A fourth layer uses global attention without positional embeddings, enabling unrestricted token interactions across the entire sequence.\n\nThe model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.\n\n", "diffllama": "## Overview\n\nThe DiffLlama model was proposed in [Differential Transformer](https://arxiv.org/abs/2410.05258) by Kazuma Matsumoto and .\nThis model is combine Llama model and Differential Transformer's Attention.\n\nThe abstract from the paper is the following:\n\n*Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.*\n\n#", "modernbert": "## Overview\n\nThe ModernBERT model was proposed in [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](https://arxiv.org/abs/2412.13663) by Benjamin Warner, Antoine Chaffin, Benjamin Clavi\u00e9, Orion Weller, Oskar Hallstr\u00f6m, Said Taghadouini, Alexis Galalgher, Raja Bisas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Grifin Adams, Jeremy Howard and Iacopo Poli.\n\nIt is a refresh of the traditional encoder architecture, as used in previous models such as [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) and [RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta). \n\nIt builds on BERT and implements many modern architectural improvements which have been developed since its original release, such as:\n- [Rotary Positional Embeddings](https://huggingface.co/blog/designing-positional-encoding) to support sequences of up to 8192 tokens.\n- [Unpadding](https://arxiv.org/abs/2208.08124) to ensure no compute is wasted on padding tokens, speeding up processing time for batches with mixed-length sequences.\n- [GeGLU](https://arxiv.org/abs/2002.05202) Replacing the original MLP layers with GeGLU layers, shown to improve performance.\n- [Alternating Attention](https://arxiv.org/abs/2004.05150v2) where most attention layers employ a sliding window of 128 tokens, with Global Attention only used every 3 layers.\n- [Flash Attention](https://github.com/Dao-AILab/flash-attention) to speed up processing.\n- A model designed following recent [The Case for Co-Designing Model Architectures with Hardware](https://arxiv.org/abs/2401.14489), ensuring maximum efficiency across inference GPUs.\n- Modern training data scales (2 trillion tokens) and mixtures (including code ande math data)\n\nThe abstract from the paper is the following:\n\n*Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.*\n\nThe original code can be found [here](https://github.com/answerdotai/modernbert).\n\n", "olmo2": "## Overview\n\nThe OLMo2 model is the successor of the OLMo model, which was proposed in\n[OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838).\n\n The architectural changes from the original OLMo model to this model are:\n\n- RMSNorm is used instead of standard layer norm.\n- Norm is applied to attention queries and keys.\n- Norm is applied after attention/feedforward layers rather than before.\n\nThis model was contributed by [shanearora](https://huggingface.co/shanearora).\nThe original code can be found [here](https://github.com/allenai/OLMo/tree/main/olmo).\n\n\n", "granitemoeshared": "## Overview\n\n\nThe GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n\nAdditionally this class GraniteMoeSharedModel adds shared experts for Moe.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"ibm-research/moe-7b-1b-active-shared-experts\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\nmodel.eval()\n\n# change input text as desired\nprompt = \"Write a code to find the maximum value in a list of numbers.\"\n\n# tokenize the text\ninput_tokens = tokenizer(prompt, return_tensors=\"pt\")\n# generate output tokens\noutput = model.generate(**input_tokens, max_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# loop over the batch to print, in this example the batch size is 1\nfor i in output:\n    print(i)\n```\n\nThis HF implementation is contributed by [Mayank Mishra](https://huggingface.co/mayank-mishra), [Shawn Tan](https://huggingface.co/shawntan) and [Sukriti Sharma](https://huggingface.co/SukritiSharma).\n\n\n"}